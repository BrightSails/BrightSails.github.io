{"categories":[{"title":"bigdata","uri":"https://brightsails.github.io/categories/bigdata/"},{"title":"error","uri":"https://brightsails.github.io/categories/error/"},{"title":"Linux","uri":"https://brightsails.github.io/categories/linux/"},{"title":"环境搭建","uri":"https://brightsails.github.io/categories/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"}],"posts":[{"content":"3.2 通过API操作HDFS 3.2.1 HDFS获取文件系统 1）详细代码\n/** * 打印本地hadoop地址值 * IO的方式写代码 */ @Test public void intiHDFS() throws IOException { //F2 可以快速的定位错误 // alt + enter自动找错误 //1.创建配信信息对象 ctrl + alt + v 后推前 ctrl + shitl + enter 补全 Configuration conf = new Configuration(); //2.获取文件系统 FileSystem fs = FileSystem.get(conf); //3.打印文件系统 System.out.println(fs.toString()); }  3.2.2 HDFS文件上传 /** * 上传代码 * 注意：如果上传的内容大于128MB,则是2块 */ @Test public void putFileToHDFS() throws Exception { //注：import org.apache.hadoop.conf.Configuration; //ctrl + alt + v 推动出对象 //1.创建配置信息对象 Configuration conf = new Configuration(); //2.设置部分参数 conf.set(\u0026quot;dfs.replication\u0026quot;,\u0026quot;2\u0026quot;); //3.找到HDFS的地址 FileSystem fs = FileSystem.get(new URI(\u0026quot;hdfs://bigdata111:9000\u0026quot;), conf, \u0026quot;root\u0026quot;); //4.上传本地Windows文件的路径 Path src = new Path(\u0026quot;D:\\\\hadoop-2.7.2.rar\u0026quot;); //5.要上传到HDFS的路径 Path dst = new Path(\u0026quot;hdfs://bigdata111:9000/\u0026quot;); //6.以拷贝的方式上传，从src -\u0026gt; dst fs.copyFromLocalFile(src,dst); //7.关闭 fs.close(); System.out.println(\u0026quot;上传成功\u0026quot;); }  3.2.3 HDFS文件下载  /** * hadoop fs -get /HDFS文件系统 * @throws Exception */ @Test public void getFileFromHDFS() throws Exception { //1.创建配置信息对象 Configuration:配置 Configuration conf = new Configuration(); //2.找到文件系统 //final URI uri ：HDFS地址 //final Configuration conf：配置信息 // String user ：Linux用户名 FileSystem fs = FileSystem.get(new URI(\u0026quot;hdfs://bigdata111:9000\u0026quot;), conf, \u0026quot;root\u0026quot;); //3.下载文件 //boolean delSrc:是否将原文件删除 //Path src ：要下载的路径 //Path dst ：要下载到哪 //boolean useRawLocalFileSystem ：是否校验文件 fs.copyToLocalFile(false,new Path(\u0026quot;hdfs://bigdata111:9000/README.txt\u0026quot;), new Path(\u0026quot;F:\\\\date\\\\README.txt\u0026quot;),true); //4.关闭fs //alt + enter 找错误 //ctrl + alt + o 可以快速的去除没有用的导包 fs.close(); System.out.println(\u0026quot;下载成功\u0026quot;); }  3.2.4 HDFS目录创建 /** * hadoop fs -mkdir /xinshou */ @Test public void mkmdirHDFS() throws Exception { //1.创新配置信息对象 Configuration configuration = new Configuration(); //2.链接文件系统 //final URI uri 地址 //final Configuration conf 配置 //String user Linux用户 FileSystem fs = FileSystem.get(new URI(\u0026quot;hdfs://bigdata111:9000\u0026quot;), configuration, \u0026quot;root\u0026quot;); //3.创建目录 fs.mkdirs(new Path(\u0026quot;hdfs://bigdata111:9000/Good/Goog/Study\u0026quot;)); //4.关闭 fs.close(); System.out.println(\u0026quot;创建文件夹成功\u0026quot;); }  3.2.5 HDFS文件夹删除 /** * hadoop fs -rm -r /文件 */ @Test public void deleteHDFS() throws Exception { //1.创建配置对象 Configuration conf = new Configuration(); //2.链接文件系统 //final URI uri, final Configuration conf, String user //final URI uri 地址 //final Configuration conf 配置 //String user Linux用户 FileSystem fs = FileSystem.get(new URI(\u0026quot;hdfs://bigdata111:9000\u0026quot;), conf, \u0026quot;root\u0026quot;); //3.删除文件 //Path var1 : HDFS地址 //boolean var2 : 是否递归删除 fs.delete(new Path(\u0026quot;hdfs://bigdata111:9000/a\u0026quot;),false); //4.关闭 fs.close(); System.out.println(\u0026quot;删除成功啦\u0026quot;); }  3.2.6 HDFS文件名更改 ````  @Test public void renameAtHDFS() throws Exception{ // 1 创建配置信息对象 Configuration configuration = new Configuration();\n FileSystem fs = FileSystem.get(new URI(\u0026quot;hdfs://bigdata111:9000\u0026quot;),configuration, \u0026quot;itstar\u0026quot;); //2 重命名文件或文件夹 fs.rename(new Path(\u0026quot;hdfs://bigdata111:9000/user/itstar/hello.txt\u0026quot;), new Path(\u0026quot;hdfs://bigdata111:9000/user/itstar/hellonihao.txt\u0026quot;)); fs.close(); } ````  3.2.7 HDFS文件详情查看  /** * 查看【文件】名称、权限等 */ @Test public void readListFiles() throws Exception { //1.创建配置对象 Configuration conf = new Configuration(); //2.链接文件系统 FileSystem fs = FileSystem.get(new URI(\u0026quot;hdfs://bigdata111:9000\u0026quot;), conf, \u0026quot;root\u0026quot;); //3.迭代器 //List the statuses and block locations of the files in the given path. If the path is a directory, if recursive is false, returns files in the directory; if recursive is true, return files in the subtree rooted at the path. If the path is a file, return the file's status and block locations. RemoteIterator\u0026lt;LocatedFileStatus\u0026gt; listFiles = fs.listFiles(new Path(\u0026quot;/\u0026quot;), true); //4.遍历迭代器 while (listFiles.hasNext()){ //一个一个出 LocatedFileStatus fileStatus = listFiles.next(); //名字 System.out.println(\u0026quot;文件名：\u0026quot; + fileStatus.getPath().getName()); //块大小 System.out.println(\u0026quot;大小：\u0026quot; + fileStatus.getBlockSize()); //权限 System.out.println(\u0026quot;权限：\u0026quot; + fileStatus.getPermission()); System.out.println(fileStatus.getLen()); BlockLocation[] locations = fileStatus.getBlockLocations(); for (BlockLocation bl:locations){ System.out.println(\u0026quot;block-offset:\u0026quot; + bl.getOffset()); String[] hosts = bl.getHosts(); for (String host:hosts){ System.out.println(host); } } System.out.println(\u0026quot;------------------华丽的分割线----------------\u0026quot;); }  3.2.8 HDFS文件和文件夹判断  /** * 判断是否是个文件还是目录，然后打印 * @throws Exception */ @Test public void judge() throws Exception { //1.创建配置文件信息 Configuration conf = new Configuration(); //2.获取文件系统 FileSystem fs = FileSystem.get(new URI(\u0026quot;hdfs://bigdata111:9000\u0026quot;), conf, \u0026quot;root\u0026quot;); //3.遍历所有的文件 //List the statuses of the files/directories in the given path if the path is a directory. FileStatus[] liststatus = fs.listStatus(new Path(\u0026quot;/\u0026quot;)); for(FileStatus status :liststatus) { //判断是否是文件 if (status.isFile()){ //ctrl + d:复制一行 //ctrl + x 是剪切一行，可以用来当作是删除一行 System.out.println(\u0026quot;文件:\u0026quot; + status.getPath().getName()); } else { System.out.println(\u0026quot;目录:\u0026quot; + status.getPath().getName()); } } }  3.3 通过IO流操作HDFS 3.3.1 HDFS文件上传 /** * IO流方式上传 * * @throws URISyntaxException * @throws FileNotFoundException * @throws InterruptedException */ @Test public void putFileToHDFSIO() throws URISyntaxException, IOException, InterruptedException { //1.创建配置文件信息 Configuration conf = new Configuration(); //2.获取文件系统 FileSystem fs = FileSystem.get(new URI(\u0026quot;hdfs://bigdata111:9000\u0026quot;), conf, \u0026quot;root\u0026quot;); //3.创建输入流 FileInputStream fis = new FileInputStream(new File(\u0026quot;F:\\\\date\\\\Sogou.txt\u0026quot;)); //4.输出路径 //注意：不能/plus 记得后边写个名 比如：/plus/Sogou.txt Path writePath = new Path(\u0026quot;hdfs://bigdata111:9000/plus/Sogou.txt\u0026quot;); FSDataOutputStream fos = fs.create(writePath); //5.流对接 //InputStream in 输入 //OutputStream out 输出 //int buffSize 缓冲区 //boolean close 是否关闭流 try { IOUtils.copyBytes(fis,fos,4 * 1024,false); } catch (IOException e) { e.printStackTrace(); }finally { IOUtils.closeStream(fos); IOUtils.closeStream(fis); fs.close(); System.out.println(\u0026quot;上传成功啦\u0026quot;); } }  3.3.2 HDFS文件下载 /** * IO读取HDFS到控制台 * * @throws URISyntaxException * @throws IOException * @throws InterruptedException */ @Test public void getFileToHDFSIO() throws URISyntaxException, IOException, InterruptedException { //1.创建配置文件信息 Configuration conf = new Configuration(); //2.获取文件系统 FileSystem fs = FileSystem.get(new URI(\u0026quot;hdfs://bigdata111:9000\u0026quot;), conf, \u0026quot;root\u0026quot;); //3.读取路径 Path readPath = new Path(\u0026quot;hdfs://bigdata111:9000/plus/Sogou.txt\u0026quot;); //4.输入 FSDataInputStream fis = fs.open(readPath); //5.输出到控制台 //InputStream in 输入 //OutputStream out 输出 //int buffSize 缓冲区 //boolean close 是否关闭流 IOUtils.copyBytes(fis,System.out,4 * 1024 ,true); }  3.3.3 定位文件读取 1）下载第一块\n/** * IO读取第一块的内容 * * @throws Exception */ @Test public void readFlieSeek1() throws Exception { //1.创建配置文件信息 Configuration conf = new Configuration(); //2.获取文件系统 FileSystem fs = FileSystem.get(new URI(\u0026quot;hdfs://bigdata111:9000\u0026quot;), conf, \u0026quot;root\u0026quot;); //3.输入 Path path = new Path(\u0026quot;hdfs://bigdata111:9000/plus/hadoop-2.7.2.rar\u0026quot;); FSDataInputStream fis = fs.open(path); //4.输出 FileOutputStream fos = new FileOutputStream(\u0026quot;F:\\\\date\\\\readFileSeek\\\\A1\u0026quot;); //5.流对接 byte[] buf = new byte[1024]; for (int i = 0; i \u0026lt; 128 * 1024; i++) { fis.read(buf); fos.write(buf); } //6.关闭流 IOUtils.closeStream(fos); IOUtils.closeStream(fis); }  2）下载第二块\n/** * IO读取第二块的内容 * * @throws Exception */ @Test public void readFlieSeek2() throws Exception { //1.创建配置文件信息 Configuration conf = new Configuration(); //2.获取文件系统 FileSystem fs = FileSystem.get(new URI(\u0026quot;hdfs://bigdata111:9000\u0026quot;), conf, \u0026quot;root\u0026quot;); //3.输入 Path path = new Path(\u0026quot;hdfs://bigdata111:9000/plus/hadoop-2.7.2.rar\u0026quot;); FSDataInputStream fis = fs.open(path); //4.输出 FileOutputStream fos = new FileOutputStream(\u0026quot;F:\\\\date\\\\readFileSeek\\\\A2\u0026quot;); //5.定位偏移量/offset/游标/读取进度 (目的：找到第一块的尾巴，第二块的开头) fis.seek(128 * 1024 * 1024); //6.流对接 IOUtils.copyBytes(fis, fos, 1024); //7.关闭流 IOUtils.closeStream(fos); IOUtils.closeStream(fis); }  3）合并文件\n在window命令窗口中执行\ntype A2 \u0026raquo; A1 然后更改后缀为rar即可\n","id":0,"section":"posts","summary":"3.2 通过API操作HDFS 3.2.1 HDFS获取文件系统 1）详细代码 /** * 打印本地hadoop地址值 * IO的方式写代码 */ @Test public void intiHDFS() throws IOException { //F2 可以快速的定位错","tags":["hdfs"],"title":"HDFS基本操作","uri":"https://brightsails.github.io/2020/02/bigdata4/","year":"2020"},{"content":"1.hadoop fs -put 命令出错 2.hadoop配置错误，需要有JAVA_HOME ","id":1,"section":"posts","summary":"1.hadoop fs -put 命令出错 2.hadoop配置错误，需要有JAVA_HOME","tags":null,"title":"Hadoop配置错误","uri":"https://brightsails.github.io/2020/02/hadoop_error/","year":"2020"},{"content":"一 HDFS概念 1.1 概念 HDFS，它是一个文件系统，**全称：Hadoop Distributed File System，用于存储文件通过目录树来定位文件；其次，它是分布式的，由很多服务器联合起来实现其功能，集群中的服务器有各自的角色。\n1.2 组成 1）HDFS集群包括，NameNode和DataNode以及Secondary Namenode。\n2）NameNode负责管理整个文件系统的元数据，以及每一个路径（文件）所对应的数据块信息。\n3）DataNode 负责管理用户的文件数据块，每一个数据块都可以在多个datanode上存储多个副本。\n4）Secondary NameNode用来监控HDFS状态的辅助后台程序，每隔一段时间获取HDFS元数据的快照。\n1.3 HDFS 文件块大小 HDFS中的文件在物理上是分块存储（block），块的大小可以通过配置参数( dfs.blocksize)来规定，默认大小在hadoop2.x版本中是128M，老版本中是64M\nHDFS的块比磁盘的块大，其目的是为了最小化寻址开销。如果块设置得足够大，从磁盘传输数据的时间会明显大于定位这个块开始位置所需的时间。因而，传输一个由多个块组成的文件的时间取决于磁盘传输速率。\n如果寻址时间约为10ms，而传输速率为100MB/s，为了使寻址时间仅占传输时间的1%，我们要将块大小设置约为100MB。默认的块大小128MB。\n块的大小：10ms100100M/s = 100M\n二 HFDS命令行操作 1）基本语法\nbin/hadoop fs 具体命令\n2）参数大全\n​ bin/hadoop fs\n[-appendToFile \u0026lt;localsrc\u0026gt; ... \u0026lt;dst\u0026gt;] [-cat [-ignoreCrc] \u0026lt;src\u0026gt; ...] [-checksum \u0026lt;src\u0026gt; ...] [-chgrp [-R] GROUP PATH...] [-chmod [-R] \u0026lt;MODE[,MODE]... | OCTALMODE\u0026gt; PATH...] [-chown [-R] [OWNER][:[GROUP]] PATH...] [-copyFromLocal [-f] [-p] \u0026lt;localsrc\u0026gt; ... \u0026lt;dst\u0026gt;] [-copyToLocal [-p] [-ignoreCrc] [-crc] \u0026lt;src\u0026gt; ... \u0026lt;localdst\u0026gt;] [-count [-q] \u0026lt;path\u0026gt; ...] [-cp [-f] [-p] \u0026lt;src\u0026gt; ... \u0026lt;dst\u0026gt;] [-createSnapshot \u0026lt;snapshotDir\u0026gt; [\u0026lt;snapshotName\u0026gt;]] [-deleteSnapshot \u0026lt;snapshotDir\u0026gt; \u0026lt;snapshotName\u0026gt;] [-df [-h] [\u0026lt;path\u0026gt; ...]] [-du [-s] [-h] \u0026lt;path\u0026gt; ...] [-expunge] [-get [-p] [-ignoreCrc] [-crc] \u0026lt;src\u0026gt; ... \u0026lt;localdst\u0026gt;] [-getfacl [-R] \u0026lt;path\u0026gt;] [-getmerge [-nl] \u0026lt;src\u0026gt; \u0026lt;localdst\u0026gt;] [-help [cmd ...]] [-ls [-d] [-h] [-R] [\u0026lt;path\u0026gt; ...]] [-mkdir [-p] \u0026lt;path\u0026gt; ...] [-moveFromLocal \u0026lt;localsrc\u0026gt; ... \u0026lt;dst\u0026gt;] [-moveToLocal \u0026lt;src\u0026gt; \u0026lt;localdst\u0026gt;] [-mv \u0026lt;src\u0026gt; ... \u0026lt;dst\u0026gt;] [-put [-f] [-p] \u0026lt;localsrc\u0026gt; ... \u0026lt;dst\u0026gt;] [-renameSnapshot \u0026lt;snapshotDir\u0026gt; \u0026lt;oldName\u0026gt; \u0026lt;newName\u0026gt;] [-rm [-f] [-r|-R] [-skipTrash] \u0026lt;src\u0026gt; ...] [-rmdir [--ignore-fail-on-non-empty] \u0026lt;dir\u0026gt; ...] [-setfacl [-R] [{-b|-k} {-m|-x \u0026lt;acl_spec\u0026gt;} \u0026lt;path\u0026gt;]|[--set \u0026lt;acl_spec\u0026gt; \u0026lt;path\u0026gt;]] [-setrep [-R] [-w] \u0026lt;rep\u0026gt; \u0026lt;path\u0026gt; ...] [-stat [format] \u0026lt;path\u0026gt; ...] [-tail [-f] \u0026lt;file\u0026gt;] [-test -[defsz] \u0026lt;path\u0026gt;] [-text [-ignoreCrc] \u0026lt;src\u0026gt; ...] [-touchz \u0026lt;path\u0026gt; ...] [-usage [cmd ...]]  3）常用命令实操\n（1）-help：输出这个命令参数\n​ bin/hdfs dfs -help rm\n（2）-ls: 显示目录信息\nhadoop fs -ls /\nHadoop fs -lsr /\n（3）-mkdir：在hdfs上创建目录\nhadoop fs -mkdir -p /hdfs路径\n（4）-moveFromLocal从本地剪切粘贴到hdfs\nhadoop fs -moveFromLocal 本地路径 /hdfs路径\n（5）\u0026ndash;appendToFile ：追加一个文件到已经存在的文件末尾\nhadoop fs -appendToFile 本地路径 /hdfs路径\n（6）-cat ：显示文件内容\n​ hadoop fs -cat /hdfs路径\n（7）-tail -f：监控文件\nhadoop fs -tail -f /hdfs路径\n（8）-chmod、-chown：linux文件系统中的用法一样，修改文件所属权限\nhadoop fs -chmod 777 /hdfs路径\nhadoop fs -chown someuser:somegrp /hdfs路径\n（9）-cp ：从hdfs的一个路径拷贝到hdfs的另一个路径\nhadoop fs -cp /hdfs路径1 / hdfs路径2\n（10）-mv：在hdfs目录中移动/重命名 文件\nhadoop fs -mv /hdfs路径 / hdfs路径\n（11）-get：等同于copyToLocal，就是从hdfs下载文件到本地\nhadoop fs -get / hdfs路径 ./本地路径\n（12）-getmerge ：合并下载多个文到linux本地，比如hdfs的目录 /aaa/下有多个文件:log.1, log.2,log.3,\u0026hellip;（注：是合成到Linux本地）\nhadoop fs -getmerge /aaa/log.* ./log.sum\n合成到不同的目录：hadoop fs -getmerge /hdfs1路径 /hdfs2路径 /\n（13）-put：等同于copyFromLocal\nhadoop fs -put /本地路径 /hdfs路径\n（14）-rm：删除文件或文件夹\nhadoop fs -rm -r /hdfs路径\n（15）-df ：统计文件系统的可用空间信息\nhadoop fs -df -h / hdfs路径\n（16）-du统计文件夹的大小信息\n[itstar@bigdata111 hadoop-2.8.4]$ hadoop fs -du -s -h / hdfs路径\n188.5 M /user/itstar/wcinput\n[itstar@bigdata111 hadoop-2.8.4]$ hadoop fs -du -h / hdfs路径\n188.5 M / hdfs路径\n97 / hdfs路径\n（17）-count：统计一个指定目录下的文件节点数量\nhadoop fs -count /aaa/\n[itstar@bigdata111 hadoop-2.8.4]$ hadoop fs -count / hdfs路径\n​ 1 2 197657784 / hdfs路径\n嵌套文件层级； 包含文件的总数\n（18）-setrep：设置hdfs中文件的副本数量：3是副本数，可改\nhadoop fs -setrep 3 / hdfs路径\n![img]这里设置的副本数只是记录在namenode的元数据中，是否真的会有这么多副本，还得看datanode的数量。因为目前只有3台设备，最多也就3个副本，只有节点数的增加到10台时，副本数才能达到10。\n3.1 IDEA环境准备 {$MAVEN_HOME/conf/settings}\n \u0026lt;!--本地仓库所在位置--\u0026gt; \u0026lt;localRepository\u0026gt;F:\\m2\\repository\u0026lt;/localRepository\u0026gt; \u0026lt;!--使用阿里云镜像去下载Jar包，速度更快--\u0026gt; \u0026lt;mirrors\u0026gt; \u0026lt;mirror\u0026gt; \u0026lt;id\u0026gt;alimaven\u0026lt;/id\u0026gt; \u0026lt;name\u0026gt;aliyun maven\u0026lt;/name\u0026gt; \u0026lt;url\u0026gt;http://maven.aliyun.com/nexus/content/groups/public/\u0026lt;/url\u0026gt; \u0026lt;mirrorOf\u0026gt;central\u0026lt;/mirrorOf\u0026gt; \u0026lt;/mirror\u0026gt; \u0026lt;/mirrors\u0026gt;  3.1.0 Maven配置 3.1.1 Maven准备 \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.hadoop\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;hadoop-common\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.8.4\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.hadoop\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;hadoop-hdfs\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.8.4\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.hadoop\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;hadoop-client\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.8.4\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.projectlombok\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;lombok\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.16.10\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;log4j\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;log4j\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.2.17\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.slf4j\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;slf4j-api\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.7.7\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- https://mvnrepository.com/artifact/junit/junit --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;junit\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;junit\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;4.12\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt;  3.1.2 IDEA准备 1）配置HADOOP_HOME环境变量\n2）采用hadoop编译后的bin 、lib两个文件夹（如果不生效，重新启动IDEA）\n3）创建第一个java工程\npublic class HdfsClientDemo1 { public static void main(String[] args) throws Exception { // 1 获取文件系统 Configuration configuration = new Configuration(); // 配置在集群上运行 configuration.set(\u0026quot;fs.defaultFS\u0026quot;, \u0026quot;hdfs://bigdata111:9000\u0026quot;); FileSystem fileSystem = FileSystem.get(configuration); // 直接配置访问集群的路径和访问集群的用户名称 //\tFileSystem fileSystem = FileSystem.get(new URI(\u0026quot;hdfs://bigdata111:9000\u0026quot;),configuration, \u0026quot;itstar\u0026quot;); // 2 把本地文件上传到文件系统中 fileSystem.copyFromLocalFile(new Path(\u0026quot;f:/hello.txt\u0026quot;), new Path(\u0026quot;/hello1.copy.txt\u0026quot;)); // 3 关闭资源 fileSystem.close(); System.out.println(\u0026quot;over\u0026quot;); } }  4）执行程序\n注：eclipse运行时可能需要配置用户名称\n客户端去操作hdfs时，是有一个用户身份的。默认情况下，hdfs客户端api会从jvm中获取一个参数来作为自己的用户身份：-DHADOOP_USER_NAME=itstar，itstar为用户名称。\n","id":2,"section":"posts","summary":"一 HDFS概念 1.1 概念 HDFS，它是一个文件系统，**全称：Hadoop Distributed File System，用于存储文件通过目录树来定位文件；其次，它是分布式的","tags":["hdfs"],"title":"HDFS概念","uri":"https://brightsails.github.io/2020/02/bigdata3/","year":"2020"},{"content":" 1、ssh  SSH 为 Secure Shell 的缩写，SSH 为建立在应用层基础上的安全协议。SSH 是目前较可靠，专为远程登录会话和其他网络服务提供安全性的协议。利用 SSH 协议可以有效防止远程管理过程中的信息泄露问题。\n 2、ssh-keygen  从客户端来看，SSH提供两种级别的安全验证：\n​ 第一种级别（基于口令的安全验证）：只要你知道自己帐号和口令，就可以登录到远程主机。所有传输的数据都会被加密，但是不能保证你正在连接的服务器就是你想连接的服务器。可能会有别的服务器在冒充真正的服务器，也就是受到“中间人”这种方式的攻击。\n​ 第二种级别（基于密钥的安全验证）ssh-keygen：需要依靠密钥，这里的密钥是非对称密钥。\n 3、t : t是type的缩写  ​ -t 即指定密钥的类型，密钥的类型有两种，一种是RSA，一种是DSA\n 4、rsa：是指RSA算法  ​ RSA：RSA加密算法是一种非对称加密算法，是由三个麻省理工的牛人弄出来的，RSA是他们三个人姓的开头首字母组合。\n​ DSA：Digital Signature Algorithm (DSA)是Schnorr和ElGamal签名算法的变种。\n为了让两个linux机器之间使用ssh不需要用户名和密码。所以采用了数字签名RSA或者DSA来完成这个操作。ssh-keygen默认使用rsa密钥，所以不加-t rsa也行，如果你想生成dsa密钥，就需要加参数-t dsa。\n 5、b ：b是bit的缩写  ​ -b 指定密钥长度。\n  6、4096\n对于RSA密钥，最小要求768位，默认是2048位。4096指的是RSA密钥长度为4096位。\nDSA密钥必须恰好是1024位(FIPS 186-2 标准的要求)。\n  7、C：C是comment的缩写\n  ​ -C表示提供一个注释，用于识别这个密钥。\n","id":3,"section":"posts","summary":"1、ssh SSH 为 Secure Shell 的缩写，SSH 为建立在应用层基础上的安全协议。SSH 是目前较可靠，专为远程登录会话和其他网络服务提供安全性的协议。利用 SSH 协","tags":null,"title":"SSH","uri":"https://brightsails.github.io/2020/02/ssh/","year":"2020"},{"content":"Hadoop的优势  高可靠性：  因为Hadoop假设计算元素和存储会出现故障，因为它维护多个工作数据副本，在出现故障时可以对失败的节点重新分布处理。\n 高扩展性：  在集群间分配任务数据，可方便的扩展数以千计的节点。\n 高效性：  在MapReduce的思想下，Hadoop是并行工作的，以加快任务处理速度。\n 高容错性：  自动保存多份副本数据，并且能够自动将失败的任务重新分配。\n  Hadoop****组成\n  Hadoop HDFS：\n  一个高可靠、高吞吐量的分布式文件系统。\n  Hadoop MapReduce：\n  一个分布式的离线并行计算框架。\n  Hadoop YARN：\n  作业调度与集群资源管理的框架。\n  Hadoop Common：\n  支持其他模块的工具模块（Configuration、RPC、序列化机制、日志操作）。\n  HDFS****架构概述\n  Namenode**：存储****元数据**\n  Datanode**：存储数据的节点，会对数据块进行校验**\n  Secondarynamenode**：监控namenode 的元数据，****每隔一定的时间进行元数据的合并**\n  YARN****架构概述\n  ResourceManager(rm)****：\n  处理客户端请求、启动/监控ApplicationMaster、监控NodeManager、资源分配与调度\n NodeManager(nm)****：  单个节点上的资源管理、处理来自ResourceManager的命令、处理来自ApplicationMaster的命令\n ApplicationMaster**：**  数据切分、为应用程序申请资源，并分配给内部任务、任务监控与容错\n Container**：**  对任务运行环境的抽象，封装了CPU、内存等多维资源以及环境变量、启动命令等任务运行相关的信息\n  MapReduce****架构概述\n  MapReduce将计算过程分为两个阶段：Map和Reduce\n  Map阶段并行处理输入数据\n  Reduce阶段对Map结果进行汇总\n  三 Hadoop运行环境搭建\n环境配置\n  关闭防火墙\n  关闭防火墙：systemctl stop firewalld.service\n  禁用防火墙：systemctl disable firewalld.service\n  查看防火墙：systemctl status firewalld.service\n  (学习阶段关闭防火墙)\n  关闭Selinux：vi /etc/selinux /config\n  将SELINUX=enforcing改为SELINUX=disabled\n  修改IP\n  善用Tab键\n  vi /etc/sysconfig/network-scripts/eno16777736\n  BOOTPROTO=static\n  ONBOOT=yes\n   IPADDR=192.168.X.51\n  GATEWAY=192.168.X.2\n  DNS1=8.8.8.8\n  DNS2=8.8.4.4\n  NETMASK=255.255.255.0\n  vi /etc/resolv.conf\n  nameserver 8.8.8.8\n  nameserver 8.8.4.4\n  重启网卡：servie network restart\n  修改主机名\n  hostnamectl set-hostname 主机名\n  IP****和主机名关系映射\n  vi /etc/hosts\n  192.168.1.121 bigdata111\n192.168.1.122 bigdata112\n192.168.1.123 bigdata113\n 在windows的C:\\Windows\\System32\\drivers\\etc路径下找到hosts并添加  192.168.1.121 bigdata111\n192.168.1.122 bigdata112\n192.168.1.123 bigdata113\n修改主机名：\nvi /etc/hostname\nbigdata111\n Xshell  输入IP、用户名和密码\n  在opt目录下创建文件（此步可选）\n  创建itstar用户\n  adduser itstar\n  passwd itstar\n  设置itstar用户具有root权限\n  vi /etc/sudoers 92行 找到root ALL=(ALL) ALL\n  复制一行：itstar ALL=(ALL) ALL\n  安装jdk\n  卸载现有jdk\n  （1）查询是否安装java软件：\nrpm -qa|grep java\n（2）如果安装的版本低于1.7，卸载该jdk：\nrpm -e 软件包名字\n 在/opt目录下创建两个子文件  mkdir /opt/mod /opt/soft\n 解压jdk到/opt/module目录下  tar -zxvf jdk-8u144-linux-x64.tar.gz -C /opt/mod/\n 配置jdk环境变量  vi /etc/profile\nexport JAVA_HOME=/opt/mod/jdk1.8.0_144 export PATH=$PATH:$JAVA_HOME/bin:$PATH :为分隔符 后面的PATH为系统的path路径 建议$PATH写前面，若自己的path写错，不影响系统PATH source /etc/profile 应用profile    测试jdk安装成功\n  java -version\n  java version \u0026ldquo;1.8.0_144\u0026rdquo;\n  四 Hadoop运行模式\n伪/完全分布式部署Hadoop\n  SSH****无密码登录\n  生成公钥和私钥：ssh-keygen -t rsa\n  然后敲（三个回车），就会生成两个文件id_rsa（私钥）、id_rsa.pub（公钥）\n  将公钥拷贝到要免密登录的目标机器上\n  ssh-copy-id 主机名1\n  ssh-copy-id 主机名2\n  ssh-copy-id 主机名3\n  注：在另外两台机器上分别执行，共执行9遍\n .ssh****文件夹下的文件功能解释  （1）~/.ssh/known_hosts ：记录ssh访问过计算机的公钥(public key)\n（2）id_rsa ：生成的私钥\n（3）id_rsa.pub ：生成的公钥\n（4）authorized_keys ：存放授权过得无秘登录服务器公钥\n 配置集群(表格版)   集群部署规划:      bigdata111 bigdata112 bigdata113     HDFS NameNode SecondaryNameNode DataNode DataNode DataNode   YARN NodeManager ResourceManager NodeManager NodeManager     配置文件：     文件 配置     core-site.xml fs.defaultFShdfs://主机名1:9000hadoop.tmp.dir/opt/module/hadoop-2.X.X/data/tmp   hdfs-site.xml dfs.replication3dfs.namenode.secondary.http-address主机名1:50090dfs.permissionsfalse   yarn-site.xml yarn.nodemanager.aux-servicesmapreduce_shuffleyarn.resourcemanager.hostname主机名1yarn.log-aggregation-enabletrueyarn.log-aggregation.retain-seconds604800   mapred-site.xml mapreduce.framework.nameyarnmapreduce.jobhistory.address主机名1:10020mapreduce.jobhistory.webapp.address主机名1:19888   hadoop-env.sh、yarn-env.sh、mapred-env.sh（分别在这些的文件中添加下面的路径） export JAVA_HOME=/opt/module/jdk1.8.0_144（注：是自己安装的路径）    slaves bigdata111、bigdata112、bigdata113（自己设置的主机名）     格式化Namenode：  hdfs namenode -format\n为什么要格式化？\nNameNode主要被用来管理整个分布式文件系统的命名空间(实际上就是目录和文件)的元数据信息，同时为了保证数据的可靠性，还加入了操作日志，所以，NameNode会持久化这些数据(保存到本地的文件系统中)。对于第一次使用HDFS，在启动NameNode时，需要先执行-format命令，然后才能正常启动NameNode节点的服务。\n格式化做了哪些事情？\n在NameNode节点上，有两个最重要的路径，分别被用来存储元数据信息和操作日志，而这两个路径来自于配置文件，它们对应的属性分别是dfs.name.dir和dfs.name.edits.dir，同时，它们默认的路径均是/tmp/hadoop/dfs/name。格式化时，NameNode会清空两个目录下的所有文件，之后，会在目录dfs.name.dir下创建文件\nhadoop.tmp.dir 这个配置，会让dfs.name.dir和dfs.name.edits.dir会让两个目录的文件生成在一个目录里\n 启动集群得命令：  namenode和ResourceManger在一台机器上：start-all.sh\nNamenode的主节点：sbin/start-dfs.sh\nYarn的主节点：sbin/start-yarn.sh\n注意：Namenode和ResourceManger如果不是同一台机器，不能在NameNode上启动 yarn，应该在ResouceManager所在的机器上启动yarn。\n scp****文件传输  实现两台远程机器之间的文件传输（bigdata112主机文件拷贝到bigdata113主机上）\nscp -r [文件] 用户@主机名：绝对路径\n注：伪分布式是一台、完全分布是三台\n 完全分布式  步骤：\n1）克隆2台客户机（关闭防火墙、静态ip、主机名称）\n2）安装jdk\n3）配置环境变量\n4）安装hadoop\n5）配置环境变量\nexport JAVA_HOME=/opt/mod/jdk1.8.0_144\nexport HADOOP_HOME=/opt/mod/hadoop-2.8.4\nexport PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME\\bin:$HADOOP_HOME\\sbin\n6）安装ssh\n7）配置集群\n8）启动测试集群\n注：此配置直接使用虚拟机克隆伪分布式两台即可\n  自带官方wordcount案例\n  随意上传一个文本文件\n  **上传命令：**hadoop fs -put 文件名 /\n  执行命令：\n  hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.X.X.jar wordcount /入 /出\n 命令解析：  hadoop jar 路径的jar包 全类名 输入路径 输出路径\n 查看结果：  hadoop fs -cat 路径\nHadoop****启动和停止命令：\n以下命令都在$HADOOP_HOME/sbin下，如果直接使用，记得配置环境变量\n   启动/停止历史服务器 mr-jobhistory-daemon.sh start|stop historyserver     启动/停止总资源管理器 yarn-daemon.sh start|stop resourcemanager   启动/停止节点管理器 yarn-daemon.sh start|stop nodemanager   启动/停止 NN 和 DN start|stop-dfs.sh   启动/停止 RN 和 NM start|stop-yarn.sh   启动/停止 NN、DN、RN、NM start|stop-all.sh   启动/停止 NN hadoop-daemon.sh start|stop namenode   启动/停止 DN hadoop-daemon.sh start|stop datanode    ","id":4,"section":"posts","summary":"Hadoop的优势 高可靠性： 因为Hadoop假设计算元素和存储会出现故障，因为它维护多个工作数据副本，在出现故障时可以对失败的节点重新分布处","tags":["hadoop"],"title":"hadoop配置","uri":"https://brightsails.github.io/2020/02/bigdata2/","year":"2020"},{"content":"Linux中Makefile设计 1.编译流程 1.预编译 gcc -E hello.c -o hello.i 目的： 1.展开宏定义 2.引入头文件，将.h引入.c 2.汇编 gcc -S hello.i -o hello.s 目的： 将C语言转换为汇编代码 3.编译 gcc -c hello.s -o hello.o 4.链接 gcc -o hello hello.o -S 汇编 -o 输出 -c 编译 linux对文件后缀没有判断  按照这个顺序来写\n使用MakeFile命令默认在当前目录下寻找文件名为MakeFile的文件\n2.设计makefile 显示规则 \t//要对生成的目标所添加的依赖 格式： target:dep cmd //makefile先检测依赖关系是否存在，若存在则执行代码，否则跳过，查询其他指令是否能达成依赖条件 hello.i:hello.c gcc -E hello.c -o hello.i  变量 OBJ = OBJ := OBJ +=\n头对象 TARGET=要生成的文件名 调用： $(文件名)\n尾对象： .PHONY: rm -rf $(变量名) 调用方法： make clean\n通配符  % 任意一个 ？ 所有 *所有 $@ 代表目标文件 $^ 代表依赖文件 $\u0026lt; 代表第一个依赖文件  ","id":5,"section":"posts","summary":"Linux中Makefile设计 1.编译流程 1.预编译 gcc -E hello.c -o hello.i 目的： 1.展开宏定义 2.引入头文件，将.h引入.c 2.汇编 gcc -S hello.i -o hello.s 目的： 将","tags":["Linux"],"title":"Makefile设计","uri":"https://brightsails.github.io/2020/02/linux1/","year":"2020"},{"content":"linux文件权限管理  共10位 r 读 w 写 x 执行 第1位为：d目录 -文件 l软连接代码 第2-5位为：文件所属者权限 第5-8位为：所属组 第8-10位为：其他用户  Linux查看命令  ls -l ==ll 查看该目录下文件的详细信息 ls -a 查看该目录下所有文件（隐藏文件） la -ls = ll+la ls -lhi 显示存储号 touch 创建文件 mv a b 将a改名为b chmod ： 文件所有者 u 同组 g 其他用户 o  chmod ug+r 文件名 chmod 777 +文件名 三位的二进制 文件所有者与同组成员增加读的权限    chown [最终用户] [文件或目录] 功能描述：改变文件或者目录的所有者\n[root@bigdata111 test1]# chown itstar test1.java [root@bigdata111 test1]# ls -al -rwxr-xr-x. 1 itstar itstar 551 5月 23 13:02 test1.java 修改前： [root@bigdata111 xiyou]# ll drwxrwxrwx. 2 root root 4096 9月 3 21:20 sunhouzi 修改后 [root@bigdata111 xiyou]# chown -R itstar:itstar sunhouzi/ [root@bigdata111 xiyou]# ll drwxrwxrwx. 2 itstar itstar 4096 9月 3 21:20 sunhouzi    vi基础操作  yy 复制一行 yNy 复制N行 p 粘贴 u 撤销 dd 删除一行 dNd 删除N行 shift+6 光标移至行头 shift+7 光标移至行尾 shift+g 光标移至最后一行 set nu 显示行号 set nonu 关闭行号 N shift+g 移至N行  Linux文件目录  / 根目录\t/root ～ 家目录 /root/username ctrl +l ，clear 清屏 mkdir 文件名 创建目录 mkdir -p aa/aa1/aa2 创建多级目录 touch 创建空文件 cp 文件名 指定目录 复制文件至指定目录 cp -r 目录名 指定目录 复制目录至指定目录 rm 文件名 删除文件（有提示，不能删除目录） rm -fr——删除文件（无提示） mv 文件名 指定目录——移动文件 cat 文件名 查看文件 more 文件名 查看文件分页 —\u0026gt;按空格——向下移动 —\u0026gt;ctrl + b——向上移动 tail -F 文件名——监控文件 echo 追加内容 \u0026raquo;文件名 \u0026ndash;\u0026gt;文件末追加内容（用于日志追踪） ctrl +c 退出监控 ln -s[源文件][目标文件] 软连接 history 历史命令  时间日期命令   data 显示当前时间\n  data -s 设置系统时间\n  cal -m 查看m月的日历\n  data +%Y 显示年份。\n（1）date -d '1 days ago'\t（功能描述：显示前一天日期） （2）date -d yesterday +%Y%m%d\t（同上） （3）date -d next-day +%Y%m%d\t（功能描述：显示明天日期） （4）date -d 'next monday'\t（功能描述：显示下周一时间）    用户管理命令   useradd 用户名 添加用户\n  passwd 密码 设置密码（root用户下）\n  userdel 删除用户\n  id 用户名 查看用户是否存在\n  su 切换用户\n  vi /etc/sudoers 修改用户权限\n  groupadd 新增用户组\n  cat etc/group 查看组\n  chown [最终用户][文件或目录]\n  chown -R 修改用户所属目录和所属组\n  who 查看登录用户信息\n（1）whoami\t（功能描述：显示自身用户名称） （2）who am i\t（功能描述：显示登录用户的用户名） （3）who\t（功能描述：看当前有哪些用户登录到了本台机器上）    设置itstar普通用户具有root权限\n修改 /etc/sudoers 文件，找到下面一行，在root下面添加一行，如下所示： ## Allow root to run any commands anywhere root ALL=(ALL) ALL itstar ALL=(ALL) ALL 或者配置成采用sudo命令时，不需要输入密码 ## Allow root to run any commands anywhere root ALL=(ALL) ALL itstar ALL=(ALL) NOPASSWD:ALL 修改完毕，现在可以用itstar帐号登录，然后用命令 su - ，即可获得root权限进行操作。    1.7 磁盘分区类 1.1.1 fdisk查看分区 1）基本语法：\n​ fdisk -l （功能描述：查看磁盘分区详情）\n​ 注意：在root用户下才能使用\n2）功能说明：\n​ （1）Linux分区\n这个硬盘是20G的，有255个磁面；63个扇区；2610个磁柱；每个 cylinder（磁柱）的容量是 8225280 bytes=8225.280 K（约为）=2.225280M（约为）；\n   Device Boot Start End Blocks Id System     分区序列 引导 从X磁柱开始 到Y磁柱结束 容量 分区类型ID 分区类型    （2）Win7分区\n3）案例\n[root@bigdata111 /]# fdisk -l\nDisk /dev/sda: 21.5 GB, 21474836480 bytes\n255 heads, 63 sectors/track, 2610 cylinders\nUnits = cylinders of 16065 * 512 = 8225280 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\nDisk identifier: 0x0005e654\nDevice Boot Start End Blocks Id System\n/dev/sda1 * 1 26 204800 83 Linux\nPartition 1 does not end on cylinder boundary.\n/dev/sda2 26 1332 10485760 83 Linux\n/dev/sda3 1332 1593 2097152 82 Linux swap / Solaris\n1.1.2 df查看硬盘 1）基本语法：\n​ df 参数 （功能描述：列出文件系统的整体磁盘使用量，检查文件系统的磁盘空间占用情况）\n参数：\n-a ：列出所有的文件系统，包括系统特有的 /proc 等文件系统；\n-k ：以 KBytes 的容量显示各文件系统；\n-m ：以 MBytes 的容量显示各文件系统；\n-h ：以人们较易阅读的 GBytes, MBytes, KBytes 等格式自行显示；\n-H ：以 M=1000K 取代 M=1024K 的进位方式；\n-T ：显示文件系统类型，连同该 partition 的 filesystem 名称 (例如 ext3) 也列出；\n-i ：不用硬盘容量，而以 inode 的数量来显示\n2）案例\n[root@bigdata111 ~]# df -h\nFilesystem Size Used Avail Use% Mounted on\n/dev/sda2 15G 3.5G 11G 26% /\ntmpfs 939M 224K 939M 1% /dev/shm\n/dev/sda1 190M 39M 142M 22% /boot\nmount/umount挂载/卸载 对于Linux用户来讲，不论有几个分区，分别分给哪一个目录使用，它总归就是一个根目录、一个独立且唯一的文件结构\nLinux中每个分区都是用来组成整个文件系统的一部分，她在用一种叫做“挂载”的处理方法，它整个文件系统中包含了一整套的文件和目录，并将一个分区和一个目录联系起来，要载入的那个分区将使它的存储空间在这个目录下获得。\n0**）挂载前准备（必须要有光盘或者已经连接镜像文件）**\n1**）挂载光盘语法：**\nmount [-t vfstype] [-o options] device dir\n（1）-t vfstype 指定文件系统的类型，通常不必指定。mount 会自动选择正确的类型。\n常用类型有：\n光盘或光盘镜像：iso9660\nDOS fat16文件系统：msdos\nWindows 9x fat32文件系统：vfat\nWindows NT ntfs文件系统：ntfs\nMount Windows文件网络共享：smbfs\nUNIX(LINUX) 文件网络共享：nfs\n（2）-o options 主要用来描述设备或档案的挂接方式。常用的参数有：\nloop：用来把一个文件当成硬盘分区挂接上系统\n　ro：采用只读方式挂接设备\n　rw：采用读写方式挂接设备\n　iocharset：指定访问文件系统所用字符集\n（3）device 要挂接(mount)的设备\n（4）dir设备在系统上的挂接点(mount point)\n2**）案例**\n（1）光盘镜像文件的挂载\n[root@bigdata111 ~]# mkdir /mnt/cdrom/ 建立挂载点\n[root@bigdata111 ~]# mount -t iso9660 /dev/cdrom /mnt/cdrom/ 设备/dev/cdrom挂载到 挂载点 ： /mnt/cdrom中\n[root@bigdata111 ~]# ll /mnt/cdrom/\n3**）卸载光盘语法：**\n[root@bigdata111 ~]# umount 设备文件名或挂载点\n4**）案例**\n[root@bigdata111 ~]# umount /mnt/cdrom\n5**）开机自动挂载语法：**\n[root@bigdata111 ~]# vi /etc/fstab\n添加红框中内容，保存退出。\n1.8 搜索查找类 1.2.2 grep 过滤查找及“|”管道符 0）管道符，“|”，表示将前一个命令的处理结果输出传递给后面的命令处理\n1）基本语法\ngrep+参数+查找内容+源文件\n参数：\n-c：只输出匹配行的计数。\n-I：不区分大小写(只适用于单字符)。\n-h：查询多文件时不显示文件名。\n-l：查询多文件时只输出包含匹配字符的文件名。\n-n：显示匹配行及行号。\n-s：不显示不存在或无匹配文本的错误信息。\n-v：显示不包含匹配文本的所有行。\n2）案例\n[root@bigdata111 opt]# ls | grep -n test\n4:test1\n5:test2\n1.2.3 which 文件搜索命令 1）基本语法：\n​ which 命令 （功能描述：搜索命令所在目录及别名信息）\n2）案例\n​ [root@bigdata111 opt]# which ls\n​ /bin/ls\n1.2.1 find 查找文件或者目录 1）基本语法：\n​ find [搜索范围] [匹配条件]\n2）案例\n（1）按文件名：根据名称查找/目录下的filename.txt文件。\n[root@bigdata111 ~]# find /opt/ -name *.txt\n（2）按拥有者：查找/opt目录下，用户名称为-user的文件\n[root@bigdata111 ~]# find /opt/ -user itstar\n​ （3）按文件大小：在/home目录下查找大于200m的文件（+n 大于 -n小于 n等于）\n[root@bigdata111 ~]find /home -size +204800\n1.9 进程线程类 进程是正在执行的一个程序或命令，每一个进程都是一个运行的实体，都有自己的地址空间，并占用一定的系统资源。\n1.9.1 ps查看系统中所有进程 1）基本语法：\n​ ps -aux （功能描述：查看系统中所有进程）\n2）功能说明\n​ USER：该进程是由哪个用户产生的\n​ PID：进程的ID号\n%CPU：该进程占用CPU资源的百分比，占用越高，进程越耗费资源；\n%MEM：该进程占用物理内存的百分比，占用越高，进程越耗费资源；\nVSZ：该进程占用虚拟内存的大小，单位KB；\nRSS：该进程占用实际物理内存的大小，单位KB；\nTTY：该进程是在哪个终端中运行的。其中tty1-tty7代表本地控制台终端，tty1-tty6是本地的字符界面终端，tty7是图形终端。pts/0-255代表虚拟终端。\nSTAT：进程状态。常见的状态有：R：运行、S：睡眠、T：停止状态、s：包含子进程、+：位于后台\nSTART：该进程的启动时间\nTIME：该进程占用CPU的运算时间，注意不是系统时间\nCOMMAND：产生此进程的命令名\n3）案例\n​ [root@bigdata111 datas]# ps -aux\n1.9.2 top查看系统健康状态 1）基本命令\n​ top [选项]\n​ （1）选项：\n​ -d 秒数：指定top命令每隔几秒更新。默认是3秒在top命令的交互模式当中可以执行的命令：\n-i：使top不显示任何闲置或者僵死进程。\n-p：通过指定监控进程ID来仅仅监控某个进程的状态。\n​ （2）操作选项：\nP： 以CPU使用率排序，默认就是此项\nM： 以内存的使用率排序\nN： 以PID排序\nq： 退出top\n​ （3）查询结果字段解释\n第一行信息为任务队列信息\n   内容 说明     12:26:46 系统当前时间   up 1 day, 13:32 系统的运行时间，本机已经运行1天 13小时32分钟   2 users 当前登录了两个用户   load average: 0.00, 0.00, 0.00 系统在之前1分钟，5分钟，15分钟的平均负载。一般认为小于1时，负载较小。如果大于1，系统已经超出负荷。    第二行为进程信息\n   Tasks: 95 total 系统中的进程总数     1 running 正在运行的进程数   94 sleeping 睡眠的进程   0 stopped 正在停止的进程   0 zombie 僵尸进程。如果不是0，需要手工检 查僵尸进程    第三行为CPU信息\n   Cpu(s): 0.1%us 用户模式占用的CPU百分比     0.1%sy 系统模式占用的CPU百分比   0.0%ni 改变过优先级的用户进程占用的CPU百分比   99.7%id 空闲CPU的CPU百分比   0.1%wa 等待输入/输出的进程的占用CPU百分比   0.0%hi 硬中断请求服务占用的CPU百分比   0.1%si 软中断请求服务占用的CPU百分比   0.0%st st（Steal time）虚拟时间百分比。就是当有虚拟机时，虚拟CPU等待实际CPU的时间百分比。    第四行为物理内存信息\n   Mem: 625344k total 物理内存的总量，单位KB     571504k used 已经使用的物理内存数量   53840k free 空闲的物理内存数量，我们使用的是虚拟机，总共只分配了628MB内存，所以只有53MB的空闲内存了   65800k buffers 作为缓冲的内存数量    第五行为交换分区（swap）信息\n   Swap: 524280k total 交换分区（虚拟内存）的总大小     0k used 已经使用的交互分区的大小   524280k free 空闲交换分区的大小   409280k cached 作为缓存的交互分区的大小    top命令第七行，各进程的监控：\n依次对应：\nPID — 进程id\nUSER — 进程所有者\nPR — 进程优先级\nNI — nice值。负值表示高优先级，正值表示低优先级\nVIRT — 进程使用的虚拟内存总量，单位kb。VIRT=SWAP+RES\nRES — 进程使用的、未被换出的物理内存大小，单位kb。RES=CODE+DATA\nSHR — 共享内存大小，单位kb\nS — 进程状态。D=不可中断的睡眠状态 R=运行 S=睡眠 T=跟踪/停止 Z=僵尸进程\n%CPU — 上次更新到现在的CPU时间占用百分比\n%MEM — 进程使用的物理内存百分比\nTIME+ — 进程使用的CPU时间总计，单位1/100秒\nCOMMAND — 进程名称（命令名/命令行）\n2）案例\n​ [root@bigdata111 itstar]# top -d 1\n[root@bigdata111 itstar]# top -i\n[root@bigdata111 itstar]# top -p 2575\n执行上述命令后，可以按P、M、N对查询出的进程结果进行排序。\n1.9.3 pstree查看进程树 1）基本语法：\n​ pstree [选项]\n​ 选项\n-p： 显示进程的PID\n-u： 显示进程的所属用户\n显示“-bash: pstree: command not found” 表示没有pstree的命令\nyum安装：\nyum -y install psmisc\n2）案例：\n​ [root@bigdata111 datas]# pstree -u\n[root@bigdata111 datas]# pstree -p\n1.9.4 kill终止进程 1）基本语法：\n​ kill -9 pid进程号\n​ 选项\n-9 表示强迫进程立即停止\n2）案例：\n​ 启动mysql程序\n​ 切换到root用户执行\n​ [root@bigdata111 桌面]# kill -9 5102\n1.9.5 netstat显示网络统计信息 1）基本语法：\n​ netstat -anp （功能描述：此命令用来显示整个系统目前的网络情况。例如目前的连接、数据包传递数据、或是路由表内容）\n​ 选项：\n​ -an 按一定顺序排列输出\n​ -p 表示显示哪个进程在调用\n​ -nltp 查看tcp协议进程端口号\n2）案例\n查看端口50070的使用情况\n[root@bigdata111 hadoop-2.1.2]# netstat -anp | grep 50070\ntcp 0 0 0.0.0.0:50070 0.0.0.0:* LISTEN 6816/java\n​ 端口号 进程号\n1.9.6 前后台进程切换 1）基本语法：\nfg %1 （功能描述：把后台进程转换成前台进程）\nctrl+z bg %1 （功能描述：把前台进程发到后台\n1.9.7 防火墙 查看：systemctl status firewalld.service\n关闭：systemctl stop firewalld.service\n禁止启动：systemctl disable firewalld.service\n1.10 压缩和解压类 1.10.1 gzip/gunzip压缩 1）基本语法：\ngzip+文件 （功能描述：压缩文件，只能将文件压缩为*.gz文件）\ngunzip+文件.gz （功能描述：解压缩文件命令）\n2）特点：\n（1）只能压缩文件不能压缩目录\n（2）不保留原来的文件\n3）案例\n（1）gzip压缩\n[root@bigdata111 opt]# ls\ntest.java\n[root@bigdata111 opt]# gzip test.java\n[root@bigdata111 opt]# ls\ntest.java.gz\n（2）gunzip解压缩文件\n[root@bigdata111 opt]# gunzip test.java.gz\n[root@bigdata111 opt]# ls\ntest.java\n1.10.2 zip/unzip压缩 1）基本语法：\nzip + 参数 + XXX.zip + 将要压缩的内容 （功能描述：压缩文件和目录的命令，window/linux通用且可以压缩目录且保留源文件）\n参数：\n-r 压缩目录\n（1）压缩 1.txt 和2.txt，压缩后的名称为mypackage.zip [root@bigdata111 opt]# zip test.zip test1.java test.java adding: test1.java (stored 0%) adding: test.java (stored 0%) [root@bigdata111 opt]# ls test1.java test.java test.zip （2）解压 mypackage.zip [root@bigdata111 opt]# unzip test.zip Archive: test.zip extracting: test1.java extracting: test.java ​ [root@bigdata111 opt]# ls test1.java test.java test.zip  1.10.3 tar打包 1）基本语法：\ntar + 参数 + XXX.tar.gz + 将要打包进去的内容 （功能描述：打包目录，压缩后的文件格式.tar.gz）\n参数：\n-c 产生.tar打包文件\n-v 显示详细信息\n-f 指定压缩后的文件名\n-z 打包同时压缩\n-x 解包.tar文件\n（1）压缩：tar -zcvf XXX.tar.gz n1.txt n2.txt ​ 压缩多个文件 [root@bigdata111 opt]# tar -zcvf test.tar.gz test1.java test.java test1.java test.java [root@bigdata111 opt]# ls test1.java test.java test.tar.gz 压缩目录 [root@bigdata111 opt]# tar -zcvf test.java.tar.gz test1 test1/ test1/hello test1/test1.java test1/test/ test1/test/test.java [root@hadoop106 opt]# ls test1 test.java.tar.gz （2）解压：tar -zxvf XXX.tar.gz ​ 解压到当前目录 [root@bigdata111 opt]# tar -zxvf test.tar.gz 解压到/opt目录 [root@bigdata111 opt]# tar -zxvf test.tar.gz -C /opt  1.11 后台服务管理类 1.11.1 service后台服务管理 1）service network status 查看指定服务的状态\n2）service network stop 停止指定服务\n3）service network start 启动指定服务\n4）service network restart 重启指定服务\n5）service \u0026ndash;status-all 查看系统中所有的后台服务\n1.11.2 chkconfig设置后台服务的自启配置 1）chkconfig 查看所有服务器自启配置\n2）chkconfig iptables off 关掉指定服务的自动启动\n3）chkconfig iptables on 开启指定服务的自动启动\n1.12 crond系统定时任务 1.12.1 crond服务管理 [root@bigdata111 ~]# service crond restart （重新启动服务）\n1.12.2 crontab定时任务设置 1）基本语法\ncrontab [选项]\n选项：\n-e： 编辑crontab定时任务\n-l： 查询crontab任务\n-r： 删除当前用户所有的crontab任务\n2）参数说明\n​ [root@bigdata111 ~]# crontab -e\n（1）进入crontab编辑界面。会打开vim编辑你的工作。\n* * * * * 执行的任务\n   项目 含义 范围     第一个“*” 一小时当中的第几分钟 0-59   第二个“*” 一天当中的第几小时 0-23   第三个“*” 一个月当中的第几天 1-31   第四个“*” 一年当中的第几月 1-12   第五个“*” 一周当中的星期几 0-7（0和7都代表星期日）    （2）特殊符号\n   特殊符号 含义     * 代表任何时间。比如第一个“*”就代表一小时中每分钟都执行一次的意思。   ， 代表不连续的时间。比如“0 8,12,16 * * * 命令”，就代表在每天的8点0分，12点0分，16点0分都执行一次命令   - 代表连续的时间范围。比如“0 5 * * 1-6命令”，代表在周一到周六的凌晨5点0分执行命令   */n 代表每隔多久执行一次。比如“*/10 * * * * 命令”，代表每隔10分钟就执行一遍命令    （3）特定时间执行命令\n   时间 含义     45 22 * * * 命令 在22点45分执行命令   0 17 * * 1 命令 每周1 的17点0分执行命令   0 5 1,15 * * 命令 每月1号和15号的凌晨5点0分执行命令   40 4 * * 1-5 命令 每周一到周五的凌晨4点40分执行命令   */10 4 * * * 命令 每天的凌晨4点，每隔10分钟执行一次命令   0 0 1,15 * 1 命令 每月1号和15号，每周1的0点0分都会执行命令。注意：星期几和几号最好不要同时出现，因为他们定义的都是天。非常容易让管理员混乱。    3）案例：\n*/5 * * * * /bin/echo ”11” \u0026raquo; /tmp/test\n*/1 * * * * /bin/echo ”11” \u0026raquo; /opt/TZ/ITSTAR\n二 RPM 2.1 概述 RPM（RedHat Package Manager），Rethat软件包管理工具，类似windows里面的setup.exe\n是Linux这系列操作系统里面的打包安装工具，它虽然是RedHat的标志，但理念是通用的。\nRPM包的名称格式\nApache-1.3.23-11.i386.rpm\n- “apache” 软件名称\n- “1.3.23-11”软件的版本号，主版本和此版本\n- “i386”是软件所运行的硬件平台\n- “rpm”文件扩展名，代表RPM包\n2.2 常用命令 2.2.1 查询（rpm -qa） 1）基本语法：\nrpm -qa （功能描述：查询所安装的所有rpm软件包）\n过滤\nrpm -qa | grep rpm软件包\n2）案例\n[root@bigdata111 Packages]# rpm -qa |grep firefox\nfirefox-45.0.1-1.el6.centos.x86_64\n2.2.2 卸载（rpm -e） 1）基本语法：\n（1）rpm -e RPM软件包\n或者（2） rpm -e \u0026ndash;nodeps 软件包\n\u0026ndash;nodeps 如果该RPM包的安装依赖其它包，即使其它包没装，也强迫安装。\n2）案例\n[root@bigdata111 Packages]# rpm -e firefox\n2.2.3 安装（rpm -ivh） 1）基本语法：\n​ rpm -ivh RPM包全名\n​ -i=install，安装\n​ -v=verbose，显示详细信息\n​ -h=hash，进度条\n​ \u0026ndash;nodeps，不检测依赖进度\n [root@bigdata111 Packages]# pwd /media/CentOS_6.8_Final/Packages [root@bigdata111 Packages]# rpm -ivh firefox-45.0.1-1.el6.centos.x86_64.rpm warning: firefox-45.0.1-1.el6.centos.x86_64.rpm: Header V3 RSA/SHA1 Signature, key ID c105b9de: NOKEY Preparing... ########################################### [100%] 1:firefox ########################################### [100%]  搜索  find[匹配范围][条件] 搜索文件 grep 管道符  进程  ps -aux 查看系统中的进程 top查看系统健康状态 kill 进程 kill -9 直接杀死进程  Linux定时任务Crontab   crontab -e 编辑定时任务\n  crontab -l 查询定时任务\n  crontable -r 删除定时任务\n  常用命令   ctrl+c 停止进程\n  ctel+l 清屏\n  tab 自动补全\n  创建硬链接 ln 文件名 链接名\n  rmdir 名字 删除空目录\n  cd - 返回上一次所在的用户\n  cd -P 跳转到实际路径\n  mv 目标名 文件名（路径） 改名，移动 若同目录下有该文件名，则改名，不同目录下，移动\n  cat -A 显示一些空白字符\n  cat -T tab键变为^I\n  cat -E 每行后加$\n  tac 倒着查看（其他属性和cat相同）\n  less + 要查看的文件 /向下查询 ？想上搜寻 q离开 上下箭头翻页\n  head -n m 文件名 显示前 m 行\n  tail -n m 文件名 显示后m行\n   重定向 例如： ls \u0026gt; test 将ls查看到的文件写入test （多次使用会覆盖）\n   ls \u0026raquo; test 追加（多次使用不会覆盖）\n  echo 文本 \u0026raquo; test 追加至test\n  echo $PATH 环境变量符号，查看环境变量\n  ","id":6,"section":"posts","summary":"linux文件权限管理 共10位 r 读 w 写 x 执行 第1位为：d目录 -文件 l软连接代码 第2-5位为：文件所属者权限 第5-8位为：所属组 第8-10位为","tags":["Linux"],"title":"Linux命令","uri":"https://brightsails.github.io/2020/02/linux2/","year":"2020"},{"content":"大数据的特征   容量（Volume）：数据的大小决定所考虑的数据的价值和潜在的信息；\n  种类（Variety）：数据类型的多样性；\n  速度（Velocity）：指获得数据的速度；\n  可变性（Variability）：妨碍了处理和有效地管理数据的过程。\n  真实性（Veracity）：数据的质量\n  复杂性（Complexity）：数据量巨大，来源多渠道\n  价值（value）：合理运用大数据，以低成本创造高价值\n  Hadoop生态   HDFS =====\u0026gt; 解决存储问题\n  MapReduce =====\u0026gt; 解决计算问题\n  Yarn =====\u0026gt; 资源协调者\n  Zookeeper =====\u0026gt; 分布式应用程序协调服务\n  Flume =====\u0026gt; 日志收集系统\n  Hive =====\u0026gt; 基于Hadoop的数仓工具\n  HBase =====\u0026gt; 分布式、面向列的开源数据库\n  Sqoop =====\u0026gt; 数据传递工具\n  Scala =====\u0026gt; 多范式编程语言、面向对象和函数式编程的特性\n  Spark =====\u0026gt; 目前企业常用的批处理离线/实时计算引擎\n  Flink =====\u0026gt; 目前最火的流处理框架、既支持流处理、也支持批处理\n  Elasticsearch =====\u0026gt; 大数据分布式弹性搜索引擎\n  Docker =====\u0026gt;Docker 是一个开源的应用容器\n  配置linux ip linux下查看ip指令：\nip addr  配置ip：\n1.进入 cd etc/sysconfig/network-scripts\n2.打开文件 vi ifcfg-ens33\n3.编辑文件 4.重新启动 service network restart  vmware及本地配置 ","id":7,"section":"posts","summary":"大数据的特征 容量（Volume）：数据的大小决定所考虑的数据的价值和潜在的信息； 种类（Variety）：数据类型的多样性； 速度（Veloci","tags":["环境搭建","Linux"],"title":"linux网络配置","uri":"https://brightsails.github.io/2020/01/bigdata1/","year":"2020"},{"content":"本地环境操作： 1.windows下载hugo环境 https://gohugo.io/getting-started/installing/  2.下载git https://www.git-scm.com/download/  3.下载node https://nodejs.org/zh-cn/download/  4.安装完成后，初始化博客 hugo new site myblog  5.新建文章 hugo new post/blog.md  6.选定主题 https://themes.gohugo.io/ 将选择的主题克隆至本地themes中，打开其中的exampleSite文件，将配置文件复制到hugo主目录  7.本地运行博客 hugo server -t pure  8.生成public文件 hugo --theme=pure --baseUrl=\u0026quot;https://brightsails.github.io\u0026quot;  9.进入文件，并且对git进行初始化，上传至github仓库 cd public/ //git初始化 git init //将文件增加至git git add . //本次提交的描述 git commit -m \u0026quot;第er次commit\u0026quot; //git用户设置 git config --global user.name\u0026quot;brightsails\u0026quot; //git用户设置 git config --global user.email\u0026quot;**********\u0026quot; //将github仓库与本地文件关联 git remote add origin https://github.com/BrightSails/brightsails.github.io.git //上传 git push -u origin master  10.更新GitHub仓库 主页面下： hugo public页面下： git add -A git commit -m \u0026quot;描述\u0026quot; git push -u origin master  11.若出现错误 To github.com:peTzxz/Property-management-system ! [rejected] master -\u0026gt; master (fetch first) error: failed to push some refs to 'git@github.com:peTzxz/Property-management-system' hint: Updates were rejected because the remote contains work that you do hint: not have locally. This is usually caused by another repository pushing hint: to the same ref. You may want to first integrate the remote changes hint: (e.g., 'git pull ...') before pushing again. hint: See the 'Note about fast-forwards' in 'git push --help' for details.  出现这个问题是因为github中的README.md文件不在本地代码目录中，可以通过如下命令进行代码合并，解决方法：\ngit pull --rebase origin master git push origin master  GitHub操作： 1.新建博客存储仓库 仓库有名字为github名字且全为小写+github.io\n2.新建图床仓库 仓库有名字随便写\n3.配置图床  下载picgo软件  https://picgo.github.io/PicGo-Doc/zh/guide/   创建token  将显示出来的token保存（只出现一次）\n 打开picgo，填入相关信息  4.图片的链接   打开GitHub建立的图床仓库，复制地址，将其中的blob改为raw，例如：\nhttps://github.com/BrightSails/pic/raw/master/hugoblog_configpic.png    博客的优化 1.pure\\layouts\\partials\\footer.html中修改：\n2.pure\\layouts\\partials\\_widgets\\board.html中修改：\n\u0026amp;\u0026amp;\n3.pure\\i18n\\zh.yaml中修改：\n","id":8,"section":"posts","summary":"\u003ch2 id=\"本地环境操作\"\u003e本地环境操作：\u003c/h2\u003e\n\u003ch4 id=\"1windows下载hugo环境\"\u003e1.windows下载hugo环境\u003c/h4\u003e\n\u003cpre\u003e\u003ccode\u003ehttps://gohugo.io/getting-started/installing/\n\u003c/code\u003e\u003c/pre\u003e","tags":null,"title":"hugo博客的搭建","uri":"https://brightsails.github.io/2020/01/hugo%E5%8D%9A%E5%AE%A2%E7%9A%84%E6%90%AD%E5%BB%BA/","year":"2020"}],"tags":[{"title":"hadoop","uri":"https://brightsails.github.io/tags/hadoop/"},{"title":"hdfs","uri":"https://brightsails.github.io/tags/hdfs/"},{"title":"Linux","uri":"https://brightsails.github.io/tags/linux/"},{"title":"环境搭建","uri":"https://brightsails.github.io/tags/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"}]}