{"categories":[{"title":"bigdata","uri":"https://brightsails.github.io/categories/bigdata/"},{"title":"C语言","uri":"https://brightsails.github.io/categories/c%E8%AF%AD%E8%A8%80/"},{"title":"error","uri":"https://brightsails.github.io/categories/error/"},{"title":"Linux","uri":"https://brightsails.github.io/categories/linux/"}],"posts":[{"content":"java 继承类的方法 package day_7_16; class p { public void pa(){ System.out.println(\u0026quot;fu pa\u0026quot;); } public void pb(){ System.out.println(\u0026quot;fu pb\u0026quot;); } public static void main(String[] args) { p p=new p(); s s=new s(); p p2=new s(); p.pa(); p.pb(); s.pa(); s.pb(); s.sa(); s.sb(); p2.pa(); p2.pb(); } } class s extends p { public void sa(){ System.out.println(\u0026quot;zi sa\u0026quot;); } public void sb(){ System.out.println(\u0026quot;zi sb\u0026quot;); } public void pa(){ System.out.println(\u0026quot;zi pa\u0026quot;); } } result: fu pa fu pb zi pa fu pb zi sa zi sb zi pa fu pb sumraise: 当父类new的值为子类时，在内存中只会装载子类中与父类共有的方法，与父类中不与子类共有的方法。  interface 方法默认权限 public static final为接口的默认访问权限\npublic abstract为默认的接口的方法的访问权限\n","id":0,"section":"posts","summary":"java 继承类的方法 package day_7_16; class p { public void pa(){ System.out.println(\u0026quot;fu pa\u0026quot;); } public void pb(){ System.out.println(\u0026quot;fu pb\u0026quot;); } public static void main(String[] args) { p p=new p(); s s=new s(); p p2=new s(); p.pa(); p.pb(); s.pa(); s.pb(); s.sa(); s.sb(); p2.pa(); p2.pb(); } } class s extends p { public void sa(){ System.out.println(\u0026quot;zi sa\u0026quot;); } public void sb(){ System.out.println(\u0026quot;zi sb\u0026quot;); } public void pa(){ System.out.println(\u0026quot;zi","tags":["Java基础"],"title":"Java 多态","uri":"https://brightsails.github.io/2020/07/java-%E5%A4%9A%E6%80%81/","year":"2020"},{"content":"vmtool安装流程\n  点击vmware 里面的虚拟机——》安装vmware tool\n  然后（等待一会）弹出一个界面把里面的 VMwareTools-9.6.1-1378637.tar.gz 复制到自己的家目录\n  解压VMwareTools-9.6.1-1378637.tar.gz　tar -xzvf VMwareTools-9.6.1-1378637.tar.gz\n  进入vmware-tools-distrib文件　cd vmware-tools-distrib\n  运行vmware-install.pl　sudo ./vmware-install.pl\n  所有配置都默认就行了，一直按enter\n  成功后会显示 welcome vmware tool\n  然后设置共享目录即可。\n  安装完后，设置共享目录\n  点击vmware里面的虚拟机——》设置\n  弹出虚拟机设置框，点击选项卡第二项“选项”\n  点击“共享文件夹”，右边出现对应详细设置\n  先点击“总是启用”，再点击“添加”，之后添加需要共享的文件夹，用默认设置就行了\n  虽然安装完了，但在ubuntu里，使用cd /mnt/hgfs 命令还是没有看到共享的目录share，并没有挂载。可用命令：df -h 查看，如下图：\n上网查了一下资料，需要先安装一个插件，具体操作和命令如下：\nsudo apt-get install open-vm-dkms sudo mount -t vmhgfs .host:/ /mnt/hgfs 可惜，分别出现了错误： E: 无法定位软件包 open-vm-dkms Error: cannot mount filesystem: No such device  第一个错误，上网查了查，大家认为是下载源的问题，可能名字不同，推荐用下面的命令逐个试一下：\nsudo apt-get install open-vm-tools open-vm-tools open-vm-tools-desktop open-vm-tools-dkms open-vm-tools-dbg open-vm-tools-dev  第二个错误，我查了很久，终于找到了答案。\n(注意下面的系统文件命令是 vmhgfs-fuse，不是vmhgfs。)\n对应的源是 open-vm-tools-dkms ，安装成功后，不能用网上大部分说的这个命令：sudo mount -t vmhgfs .host:/ /mnt/hgfs ，这个命令是不行的，我想这个命令大概对应源名字为：open-vm-tools。tools的名字换了可能命令的名字也不一样，正确的命令是：sudo vmhgfs-fuse .host:/ /mnt/hgfs 。终于可以挂载成功了！挂载成功的截图：\n","id":1,"section":"posts","summary":"vmtool安装流程 点击vmware 里面的虚拟机——》安装vmware tool 然后（等待一会）弹出一个界面把里面的 VMwareTools-9.6.1-1378637.tar.gz 复制到自己的家目录 解压VMwa","tags":["环境搭建"],"title":"linux共享文件夹设置","uri":"https://brightsails.github.io/2020/04/linux-share-files/","year":"2020"},{"content":"vscode配置： 1.插件安装  remote developmen SFTP  2.文件配置 setting.json中添加：\n\u0026quot;remote.SSH.showLoginTerminal\u0026quot;: true  config中设置：\nHost brightsails HostName 192.168.1.130 //虚拟机ip User brightsails //要登陆的用户  windows配置 在cmd中输入：\nssh-keygen -t rsa  Ubuntu设置 ssh配置 将在windows中获得的公匙传入linux的~/.ssh下，~为需要远程登陆的用户(可以直接将公匙文件复制到该目录下)，创建文件authorized_keys，然后执行\ncat id_rsa.pub \u0026gt;\u0026gt; authorized_keys  配置密匙的原因：为了免密登录，如果不需要免密登录，可以不配置。\nsshd_config配置 添加： RSAAuthentication yes PubkeyAuthentication yes  勘误 1.若在vscode中不能保存文件，原因是linux中文件权限的问题，修改linux中的文件权限即可\n","id":2,"section":"posts","summary":"vscode配置： 1.插件安装 remote developmen SFTP 2.文件配置 setting.json中添加： \u0026quot;remote.SSH.showLoginTerminal\u0026quot;: true config中设置： Host brightsails HostName 192.168.1.130 //虚拟机ip User brightsails //要登陆","tags":["环境搭建"],"title":"Vscode连接linux","uri":"https://brightsails.github.io/2020/04/vscode_config/","year":"2020"},{"content":"在传递一级指针时，只有对指针所指向的内存变量做操作才是有效的；\n在传递二级指针时，只有对指针的指向做改变才是有效的；\n下面做简单的分析：\n在函数传递参数时，编译器总会为每个函数参数制作一个副本，即拷贝；\n例如：\nvoid fun(int *p)，指针参数p的副本为_p，编译器使_p=p，_p和p指向相同的内存空间，如果在函数内修改了_p所指向的内容，就会导致p的内容也做相应的改变；\n但如果在函数内_p申请了新的内存空间或者指向其他内存空间，则_p指向了新的内存空间，而p依旧指向原来的内存空间，因此函数返回后p还是原来的p。\n这样的话，不但没有实现功能，反而每次都申请新的内存空间，而又得不到释放，因为没有将该内存空间的地址传递出来，容易造成内存泄露。\nvoid fun(int **p)，如果函数参数是指针的地址，则可以通过该参数p将新分配或新指向的内存地址传递出来，这样就实现了有效的指针操作。\n#include\u0026lt;stdio.h\u0026gt; #include\u0026lt;malloc.h\u0026gt; typedef struct list { int data; }list_t, *list_pt; void pp(list_t ** x) { // 此时 x是\u0026amp;tmp拷贝，直接输出x的值为tmp的地址 printf(\u0026quot;\u0026amp;x=%d\\n\u0026quot;, \u0026amp;x); // 打印x的地址，与tmp的地址不一样 printf(\u0026quot;x=%d\\n\u0026quot;, x); // 输出x的值为tmp的地址 printf(\u0026quot;*x=%d\\n\u0026quot;, *x); // 输出的为tmp的值，也就是结构体内存的首地址 printf(\u0026quot;**x=%d\\n\u0026quot;, **x); // 输出的为结构体的首地址为起始的int长度的值 int tmp1 = 5; x = \u0026amp;tmp1; //此时x指向的值是新的tmp1 ，当此处x为*x时，即为tmp的指向变为了tmp1的地址 printf(\u0026quot;%d\\n\u0026quot;,*x); } int main() { list_pt tmp = NULL; tmp = (list_pt)malloc(sizeof(list_t)); tmp-\u0026gt;data = 10; printf(\u0026quot;tmp=%d\\n\u0026quot;, tmp); printf(\u0026quot;\u0026amp;tmp=%d\\n\u0026quot;, \u0026amp;tmp); //打印tmp指针的地址 pp(\u0026amp;tmp); //传入tmp的地址 printf(\u0026quot;%d\\n\u0026quot;, *tmp); return 0; }  ","id":3,"section":"posts","summary":"在传递一级指针时，只有对指针所指向的内存变量做操作才是有效的； 在传递二级指针时，只有对指针的指向做改变才是有效的； 下面做简单的分析： 在函数传","tags":null,"title":"二级指针","uri":"https://brightsails.github.io/2020/04/%E4%BA%8C%E7%BA%A7%E6%8C%87%E9%92%88/","year":"2020"},{"content":"通过API操作HDFS package HdfsAPI; import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.fs.*; import org.apache.hadoop.io.IOUtils; import org.junit.After; import org.junit.Before; import org.junit.Test; import java.io.*; import java.net.URI; import java.net.URISyntaxException; import static org.apache.hadoop.fs.FileSystem.*; public class HdfsAPI { //实例化hadoop配置文件 //后推前 ctrl+alt+v //去除多余的包 ctrl+calt+o //提示 ctrl+alt+o //@Test junit test FileSystem fs; Configuration conf; @Test public void initHdfsToString() throws IOException{ conf = new Configuration(); //get filesystem fs= get(conf); //print filesystem System.out.println(fs.toString()); } @Before public void initHdfs() throws URISyntaxException, IOException, InterruptedException { //实例化配置 conf=new Configuration(); //获取文件系统 fs=get(new URI(\u0026quot;hdfs://bigdata111:9000\u0026quot;), conf, \u0026quot;root\u0026quot;); //System.out.println(\u0026quot;before running\u0026quot;); } @After public void closeHDFs() throws IOException{ fs.close(); //System.out.println(\u0026quot;after running\u0026quot;); } @Test /** * upload */ public void putFileHdfs() throws URISyntaxException, IOException, InterruptedException { // //1.实例化配置文件 // Configuration conf = new Configuration(); //设置参数(dfs.replication为配置文件中的冗余name和value) conf.set(\u0026quot;dfs.replication\u0026quot;,\u0026quot;2\u0026quot;); //2.获取文件系统 FileSystem fs = get(new URI(\u0026quot;hdfs://bigdata111:9000\u0026quot;), conf, \u0026quot;root\u0026quot;); //3.定义文件输入输出的路径 Path in = new Path(\u0026quot;D:/text/z.txt\u0026quot;); Path out = new Path(\u0026quot;hdfs://bigdata111:9000/\u0026quot;); //4.上传 fs.copyFromLocalFile(in,out); // //5.关闭文件系统 // fs.close(); System.out.println(\u0026quot;upload success\u0026quot;); } /** * 下载 */ @Test public void getFileHdfs() throws URISyntaxException, IOException, InterruptedException { // //实例化配置 // Configuration conf=new Configuration(); // //获取文件系统 // FileSystem fs=get(new URI(\u0026quot;hdfs://bigdata111:9000\u0026quot;), conf, \u0026quot;root\u0026quot;); //定义下载地址，hdfs Path src = new Path(\u0026quot;/english.txt\u0026quot;); //下载到win的路径 Path out = new Path(\u0026quot;D:\\\\text\\\\test.txt\u0026quot;); //download //The first parameter---delSrc--\u0026gt;whether delete source file //The second parameter---hdfs path //The third parameter---win path //The final parameter---useRawLocalFileSystem--\u0026gt;whether verification file completeness fs.copyToLocalFile(false,src,out,true); // fs.close(); System.out.println(\u0026quot;download success\u0026quot;); } @Test /** * Create catalogue */ public void mkidrHdfsFile() throws URISyntaxException, IOException, InterruptedException { // //实例化配置 // Configuration conf=new Configuration(); // //获取文件系统 // FileSystem fs=get(new URI(\u0026quot;hdfs://bigdata111:9000\u0026quot;), conf, \u0026quot;root\u0026quot;); //Hdfs path Path path = new Path(\u0026quot;/0310\u0026quot;); //Create catalogue fs.mkdirs(path); // fs.close(); System.out.println(\u0026quot;Create success\u0026quot;); } @Test /** * delete catalouge */ public void rmHdfsFile() throws IOException { //delete hdfs path Path path = new Path(\u0026quot;/0310\u0026quot;); //delete //delete file false ,catalogue true(是否递归) fs.delete(path,false); System.out.println(\u0026quot;delete success\u0026quot;); } @Test /** * rename */ public void reName() throws IOException { //The first parameter--\u0026gt;hdfs name fs.rename(new Path(\u0026quot;/qqq.txt\u0026quot;), new Path(\u0026quot;/aaa.txt\u0026quot;)); System.out.println(\u0026quot;reName success\u0026quot;); } @Test /** * check catalogue or file detailed information */ public void readFileHdfs() throws IOException { //attain catalogue's all file iterator RemoteIterator\u0026lt;LocatedFileStatus\u0026gt; listfiles = fs.listFiles(new Path(\u0026quot;/\u0026quot;), true); while (listfiles.hasNext()){ //get every file object LocatedFileStatus file = listfiles.next(); System.out.println(\u0026quot;fileName:\u0026quot;+file.getPath().getName()+ \u0026quot;\\n\u0026quot;+\u0026quot;Blocksize:\u0026quot;+file.getBlockSize()+\u0026quot;\\n\u0026quot;+\u0026quot;filesize:\u0026quot;+file.getLen()); //every block size BlockLocation[] blockLocations = file.getBlockLocations(); for (BlockLocation block:blockLocations){ //偏移量 System.out.println(\u0026quot;Offset:\u0026quot;+block.getOffset()); String[] hosts = block.getHosts(); for(String s:hosts){ System.out.println(hosts); } } System.out.println(\u0026quot;=======================\u0026quot;); } } @Test /** * judge whether is file */ public void isFile() throws IOException { //attain catalogue all path's object FileStatus[] fileStatuses = fs.listStatus(new Path(\u0026quot;/\u0026quot;)); for (FileStatus s:fileStatuses){ if(s.isFile()){ System.out.println(s.getPath().getName()+\u0026quot; is file\u0026quot;); }else{ System.out.println(s.getPath().getName()+\u0026quot; is directory\u0026quot;); } } } /** * IO流方式上传 */ @Test public void putFileToHDFSIO() throws IOException{ FileInputStream fis = new FileInputStream(new File(\u0026quot;d:/text/qqq.txt\u0026quot;)); //2.输出路径 //注意：不能/plus,记得后边写个名 比如：:/text/qqq.txt Path writePath = new Path(\u0026quot;hdfs://bigdata111:9000/qqq.txt\u0026quot;); FSDataOutputStream fos = fs.create(writePath); //3.流对接 //InputStream in 输入 //OutputStream out 输出 //int buffSize 缓冲区 //boolean close 是否关闭流 try { IOUtils.copyBytes(fis,fos,4 * 1024,false); } catch (IOException e) { e.printStackTrace(); }finally { IOUtils.closeStream(fos); IOUtils.closeStream(fis); fs.close(); System.out.println(\u0026quot;upload success\u0026quot;); } } @Test /** * download */ public void getFileHdfsIO() throws IOException { //Create input stream FSDataInputStream fis=fs.open(new Path(\u0026quot;/person\u0026quot;)); //Create otput stream FileOutputStream fos=new FileOutputStream(new File(\u0026quot;D:/text/person.txt\u0026quot;)); //Create mutual flow //set true auto close flow IOUtils.copyBytes(fis,fos,4 * 1024,true); System.out.println(\u0026quot;download success\u0026quot;); } @Test /** * Location reading the fist file block */ public void readFileBloack1() throws IOException { //Create input stream FSDataInputStream fis=fs.open(new Path(\u0026quot;/z.txt\u0026quot;)); //Create otput stream FileOutputStream fos=new FileOutputStream(new File(\u0026quot;D:/text/x.txt\u0026quot;)); //create mutual flow byte[] buff = new byte[1024]; for (int i=0;i\u0026lt;1;i++){ fis.read(buff); fos.write(buff); } IOUtils.closeStream(fis); IOUtils.closeStream(fos); fs.close(); } @Test /** * location reading the second file block */ public void readFileBloack2() throws IOException { //Create input stream FSDataInputStream fis=fs.open(new Path(\u0026quot;/z.txt\u0026quot;)); //Create otput stream FileOutputStream fos=new FileOutputStream(new File(\u0026quot;D:/text/x.txt\u0026quot;)); //location offset fis.seek(128*1024*1024); ///create mutual stream IOUtils.copyBytes(fis,fos,1024,true); fs.close(); } }  合并文件 在window命令窗口中执行，type A2 \u0026raquo; A1 然后更改后缀为rar即可。\n","id":4,"section":"posts","summary":"通过API操作HDFS package HdfsAPI; import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.fs.*; import org.apache.hadoop.io.IOUtils; import org.junit.After; import org.junit.Before; import org.junit.Test; import java.io.*; import java.net.URI; import java.net.URISyntaxException; import static org.apache.hadoop.fs.FileSystem.*; public class HdfsAPI { //实例化hadoop配置文件 //后推前 ctrl+alt+v //去除多余的包 ctrl+calt+o //","tags":["hdfs"],"title":"API操作HDFS","uri":"https://brightsails.github.io/2020/03/hdfsmethod/","year":"2020"},{"content":"GDB安装 apt-get install gdb  安装GDB增强工具 (gef)  GDB的版本大于7.7 wget -q -O- https://github.com/hugsy/gef/raw/master/scripts/gef.sh | sh 确保网络连通 并且成功更新ubuntu (更新source.list 使用apt-get update)  安装GDB增强工具 (pwndbg)   git clone https://github.com/pwndbg/pwndbg\n  cd pwndbg\n  sudo #./setup.sh\n  GDB启动方式  gdb 文件名 gdb 文件名 core（错误文件日志） set args -c 0 -i 123456  readelf -h查看文件头 其中的entry point address为程序起始地址  GDB调试 1.基于源码的调试 基础知识  RIP 执行指针 ESP 栈顶指针 EAX当前栈指针位置 EBP基址指针  基础指令：   vmmap 程序中关键地址段\n  nexti 汇编下一行\n  n 下一行\n  u 跳出循环\np/x变量名 查看地址 p/d 以十进制打印值 p *变量 查看结构体，地址 p *(db_list_t * )变量名 p \u0026amp;变量名 查看该变量地址 p 变量名=x 直接复制    s 进入函数内\n  b 行数 跳转\n  b 函数名 跳转\n  set args 参数\u0026hellip; 设置参数\n  l main 查看main函数\n  break （b） 函数名，行号 打断点\n  shell 跳转至shell\n  exit 退出shell\n  r 运行\n  x/20wx（32位，gz为64位）查看20个16 进制的字节\nx/s 以字符串方式查看 x/sa 变量名 查看字符串 s为字长 b 比特 w 4字节 g 8字节 a为进制 d是十进制 f,z十六进制 x八进制    i r查看所有的寄存器的值\n  bt 查看进入的函数的调用关系\n  info frame 0 查看当前函数调用栈\n  x/i 查看二进制代码\n  l 名字 查看具体代码\n  c 跳至下一断点\n  b 变量 if 条件 条件断点\n  条件断点 若在for中循环100次，需要在第50次循环停下来\nb 行号 if （i==50）\n多进程 follow-fork-mode 设置进程模式\ndetach-on-fork 是否只调试子进程\n防破解的保护机制   ASLR与PIE ASLP为系统层面，ASPR为指令地址随机化（不开启偏移地址是固定的），PIE是在GCC编译时自行决定是否开启\n  Canary 栈保护机制，在函数返回时校验cookie，从而判断是否栈溢出\n  NX 栈不可执行\n  RELRO\n  加壳\n  段错误（核心已转储） 查看core文件，设置方法：切换至root下，输入**”ulimit -c unlimited “**命令。\n在gdb中添加core文件去调试程序：gdb mult_proxy core\nnc-lvp 端口号 监听端口\n","id":5,"section":"posts","summary":"GDB安装 apt-get install gdb 安装GDB增强工具 (gef) GDB的版本大于7.7 wget -q -O- https://github.com/hugsy/gef/raw/master/scripts/gef.sh | sh 确保网络连通 并且成功更新ubuntu (更新source.list 使用a","tags":["GDB"],"title":"GDB基础","uri":"https://brightsails.github.io/2020/03/gdb/","year":"2020"},{"content":"Source insight安装 1.安装并配置 文件准备：  mycomment.em 做注释 openfloder.em 快速打开目标目录 quicker.em 快捷键集合  操作： 1.环境准备以及破解  将sourceinsight4以及三个宏文件放入下列目录   导入破解文件   将配置xml文件放入source insight的settings下  2.打开基础工程： 3.配置  打开基础工程   添加宏文件   同步宏文件   加载配置文件，选择fatal.xml即可  4.新建项目 先在要打开的工程中新建一个文件，在source insight中新建一个工程，路径选择在刚刚建文件的目录下面（必须为全英文）。\n​\t点击确定，然后把需要的.c文件选中 ​\t然后同步即可。 5.设置\n 工作区布局设置   snippt功能：存储代码片段   编码设置  6.快捷键\n  ctrl+3 宏注释\n  ctrl+1 快速注释\n  ctrl+8 多行注释\n  ctrl+t 快速定位到目录\n  ","id":6,"section":"posts","summary":"Source insight安装 1.安装并配置 文件准备： mycomment.em 做注释 openfloder.em 快速打开目标目录 quicker.em 快捷键集合 操作： 1.环境准备以及破解 将sourceinsight4以","tags":["环境搭建"],"title":"Source安装","uri":"https://brightsails.github.io/2020/03/source%E5%AE%89%E8%A3%85/","year":"2020"},{"content":"1.ubuntu xrandr显示问题 ​\txrandr no protocol specified\n​\t用户问题，使用默认登录用户即可解决。\n2.ubuntu 分辨率修改 ​\txrandr -s 1920*1200 全屏\n3.ubuntu 配置好后无法ssh登录 修改ssh改配置文件，设置为允许root远程登录： root@ubuntu:~# vim /etc/ssh/sshd_config 将PermitRootLogin prohibie-password 修改为：PermitRootLogin yes 即可 保存退出，重启ssh服务： root@ubuntu:~# /etc/init.d/ssh restart 再次尝试ssh 远程登录，成功登录  4.安装termcap termcap是一个终端数据库，termcap不需要交叉编译，在Ubuntu下直接编译即可，我的做法是： 首先在网上从各种垃圾下载中找到了一个真正能用的termcap 安装包，链接为点击打开链接\n然后将该文件直接解压出来， 执行配置脚本\n ./configure --target=arm-none-linux-gnueabi --prefix=/opt/termcap  然后执行make和make install安装即可。让我很折腾的是网上说该文件要放在交叉编译工具链的文件夹下，其实不对，只要将/opt/termcap/include/termcap.h复制到/usr/include文件夹下，将/opt/termcap/lib/libtermcap.a复制到/usr/lib目录下即可**(也可添加和环境变量)**\n然后重新执行build-all就安装完了，文件安装到了/opt/arm-linux-gdb下，我测试了一下是正确的。\n最后，配置环境变量，为了更方便的调用，这里将arm-linux-gdb设置到PATH环境变量中，方法是\nroot权限下执行gedit /etc/bash.bashrc/，然后在文件的末尾加上下面这行代码 export PATH=$PATH:/opt/arm-linux-gdb/bin export PATH=$PATH:/opt/FriendlyARM/toolschain/4.4.3/bin  注意，这里如果有arm-linux-gcc的设置，一定要将arm-linux-gdb的放在上面，因为有些gcc中自带gdb，如果反过来就不能使用我们安装的了。 最后执行下面的命令\nsource /etc/bash.bashrc  ","id":7,"section":"posts","summary":"1.ubuntu xrandr显示问题 ​ xrandr no protocol specified ​ 用户问题，使用默认登录用户即可解决。 2.ubuntu 分辨率修改 ​ xrandr -s 1920*1200 全屏 3.ubuntu 配置好后无法ssh登录 修改ssh改配置文件，","tags":null,"title":"Linux_error","uri":"https://brightsails.github.io/2020/03/linux_error/","year":"2020"},{"content":"3.2 通过API操作HDFS 3.2.1 HDFS获取文件系统 1）详细代码\n/** * 打印本地hadoop地址值 * IO的方式写代码 */ @Test public void intiHDFS() throws IOException { //F2 可以快速的定位错误 // alt + enter自动找错误 //1.创建配信信息对象 ctrl + alt + v 后推前 ctrl + shitl + enter 补全 Configuration conf = new Configuration(); //2.获取文件系统 FileSystem fs = FileSystem.get(conf); //3.打印文件系统 System.out.println(fs.toString()); }  3.2.2 HDFS文件上传 /** * 上传代码 * 注意：如果上传的内容大于128MB,则是2块 */ @Test public void putFileToHDFS() throws Exception { //注：import org.apache.hadoop.conf.Configuration; //ctrl + alt + v 推动出对象 //1.创建配置信息对象 Configuration conf = new Configuration(); //2.设置部分参数 conf.set(\u0026quot;dfs.replication\u0026quot;,\u0026quot;2\u0026quot;); //3.找到HDFS的地址 FileSystem fs = FileSystem.get(new URI(\u0026quot;hdfs://bigdata111:9000\u0026quot;), conf, \u0026quot;root\u0026quot;); //4.上传本地Windows文件的路径 Path src = new Path(\u0026quot;D:\\\\hadoop-2.7.2.rar\u0026quot;); //5.要上传到HDFS的路径 Path dst = new Path(\u0026quot;hdfs://bigdata111:9000/\u0026quot;); //6.以拷贝的方式上传，从src -\u0026gt; dst fs.copyFromLocalFile(src,dst); //7.关闭 fs.close(); System.out.println(\u0026quot;上传成功\u0026quot;); }  3.2.3 HDFS文件下载  /** * hadoop fs -get /HDFS文件系统 * @throws Exception */ @Test public void getFileFromHDFS() throws Exception { //1.创建配置信息对象 Configuration:配置 Configuration conf = new Configuration(); //2.找到文件系统 //final URI uri ：HDFS地址 //final Configuration conf：配置信息 // String user ：Linux用户名 FileSystem fs = FileSystem.get(new URI(\u0026quot;hdfs://bigdata111:9000\u0026quot;), conf, \u0026quot;root\u0026quot;); //3.下载文件 //boolean delSrc:是否将原文件删除 //Path src ：要下载的路径 //Path dst ：要下载到哪 //boolean useRawLocalFileSystem ：是否校验文件 fs.copyToLocalFile(false,new Path(\u0026quot;hdfs://bigdata111:9000/README.txt\u0026quot;), new Path(\u0026quot;F:\\\\date\\\\README.txt\u0026quot;),true); //4.关闭fs //alt + enter 找错误 //ctrl + alt + o 可以快速的去除没有用的导包 fs.close(); System.out.println(\u0026quot;下载成功\u0026quot;); }  3.2.4 HDFS目录创建 /** * hadoop fs -mkdir /xinshou */ @Test public void mkmdirHDFS() throws Exception { //1.创新配置信息对象 Configuration configuration = new Configuration(); //2.链接文件系统 //final URI uri 地址 //final Configuration conf 配置 //String user Linux用户 FileSystem fs = FileSystem.get(new URI(\u0026quot;hdfs://bigdata111:9000\u0026quot;), configuration, \u0026quot;root\u0026quot;); //3.创建目录 fs.mkdirs(new Path(\u0026quot;hdfs://bigdata111:9000/Good/Goog/Study\u0026quot;)); //4.关闭 fs.close(); System.out.println(\u0026quot;创建文件夹成功\u0026quot;); }  3.2.5 HDFS文件夹删除 /** * hadoop fs -rm -r /文件 */ @Test public void deleteHDFS() throws Exception { //1.创建配置对象 Configuration conf = new Configuration(); //2.链接文件系统 //final URI uri, final Configuration conf, String user //final URI uri 地址 //final Configuration conf 配置 //String user Linux用户 FileSystem fs = FileSystem.get(new URI(\u0026quot;hdfs://bigdata111:9000\u0026quot;), conf, \u0026quot;root\u0026quot;); //3.删除文件 //Path var1 : HDFS地址 //boolean var2 : 是否递归删除 fs.delete(new Path(\u0026quot;hdfs://bigdata111:9000/a\u0026quot;),false); //4.关闭 fs.close(); System.out.println(\u0026quot;删除成功啦\u0026quot;); }  3.2.6 HDFS文件名更改 ````  @Test public void renameAtHDFS() throws Exception{ // 1 创建配置信息对象 Configuration configuration = new Configuration();\n FileSystem fs = FileSystem.get(new URI(\u0026quot;hdfs://bigdata111:9000\u0026quot;),configuration, \u0026quot;itstar\u0026quot;); //2 重命名文件或文件夹 fs.rename(new Path(\u0026quot;hdfs://bigdata111:9000/user/itstar/hello.txt\u0026quot;), new Path(\u0026quot;hdfs://bigdata111:9000/user/itstar/hellonihao.txt\u0026quot;)); fs.close(); } ````  3.2.7 HDFS文件详情查看  /** * 查看【文件】名称、权限等 */ @Test public void readListFiles() throws Exception { //1.创建配置对象 Configuration conf = new Configuration(); //2.链接文件系统 FileSystem fs = FileSystem.get(new URI(\u0026quot;hdfs://bigdata111:9000\u0026quot;), conf, \u0026quot;root\u0026quot;); //3.迭代器 //List the statuses and block locations of the files in the given path. If the path is a directory, if recursive is false, returns files in the directory; if recursive is true, return files in the subtree rooted at the path. If the path is a file, return the file's status and block locations. RemoteIterator\u0026lt;LocatedFileStatus\u0026gt; listFiles = fs.listFiles(new Path(\u0026quot;/\u0026quot;), true); //4.遍历迭代器 while (listFiles.hasNext()){ //一个一个出 LocatedFileStatus fileStatus = listFiles.next(); //名字 System.out.println(\u0026quot;文件名：\u0026quot; + fileStatus.getPath().getName()); //块大小 System.out.println(\u0026quot;大小：\u0026quot; + fileStatus.getBlockSize()); //权限 System.out.println(\u0026quot;权限：\u0026quot; + fileStatus.getPermission()); System.out.println(fileStatus.getLen()); BlockLocation[] locations = fileStatus.getBlockLocations(); for (BlockLocation bl:locations){ System.out.println(\u0026quot;block-offset:\u0026quot; + bl.getOffset()); String[] hosts = bl.getHosts(); for (String host:hosts){ System.out.println(host); } } System.out.println(\u0026quot;------------------华丽的分割线----------------\u0026quot;); }  3.2.8 HDFS文件和文件夹判断  /** * 判断是否是个文件还是目录，然后打印 * @throws Exception */ @Test public void judge() throws Exception { //1.创建配置文件信息 Configuration conf = new Configuration(); //2.获取文件系统 FileSystem fs = FileSystem.get(new URI(\u0026quot;hdfs://bigdata111:9000\u0026quot;), conf, \u0026quot;root\u0026quot;); //3.遍历所有的文件 //List the statuses of the files/directories in the given path if the path is a directory. FileStatus[] liststatus = fs.listStatus(new Path(\u0026quot;/\u0026quot;)); for(FileStatus status :liststatus) { //判断是否是文件 if (status.isFile()){ //ctrl + d:复制一行 //ctrl + x 是剪切一行，可以用来当作是删除一行 System.out.println(\u0026quot;文件:\u0026quot; + status.getPath().getName()); } else { System.out.println(\u0026quot;目录:\u0026quot; + status.getPath().getName()); } } }  3.3 通过IO流操作HDFS 3.3.1 HDFS文件上传 /** * IO流方式上传 * * @throws URISyntaxException * @throws FileNotFoundException * @throws InterruptedException */ @Test public void putFileToHDFSIO() throws URISyntaxException, IOException, InterruptedException { //1.创建配置文件信息 Configuration conf = new Configuration(); //2.获取文件系统 FileSystem fs = FileSystem.get(new URI(\u0026quot;hdfs://bigdata111:9000\u0026quot;), conf, \u0026quot;root\u0026quot;); //3.创建输入流 FileInputStream fis = new FileInputStream(new File(\u0026quot;F:\\\\date\\\\Sogou.txt\u0026quot;)); //4.输出路径 //注意：不能/plus 记得后边写个名 比如：/plus/Sogou.txt Path writePath = new Path(\u0026quot;hdfs://bigdata111:9000/plus/Sogou.txt\u0026quot;); FSDataOutputStream fos = fs.create(writePath); //5.流对接 //InputStream in 输入 //OutputStream out 输出 //int buffSize 缓冲区 //boolean close 是否关闭流 try { IOUtils.copyBytes(fis,fos,4 * 1024,false); } catch (IOException e) { e.printStackTrace(); }finally { IOUtils.closeStream(fos); IOUtils.closeStream(fis); fs.close(); System.out.println(\u0026quot;上传成功啦\u0026quot;); } }  3.3.2 HDFS文件下载 /** * IO读取HDFS到控制台 * * @throws URISyntaxException * @throws IOException * @throws InterruptedException */ @Test public void getFileToHDFSIO() throws URISyntaxException, IOException, InterruptedException { //1.创建配置文件信息 Configuration conf = new Configuration(); //2.获取文件系统 FileSystem fs = FileSystem.get(new URI(\u0026quot;hdfs://bigdata111:9000\u0026quot;), conf, \u0026quot;root\u0026quot;); //3.读取路径 Path readPath = new Path(\u0026quot;hdfs://bigdata111:9000/plus/Sogou.txt\u0026quot;); //4.输入 FSDataInputStream fis = fs.open(readPath); //5.输出到控制台 //InputStream in 输入 //OutputStream out 输出 //int buffSize 缓冲区 //boolean close 是否关闭流 IOUtils.copyBytes(fis,System.out,4 * 1024 ,true); }  3.3.3 定位文件读取 1）下载第一块\n/** * IO读取第一块的内容 * * @throws Exception */ @Test public void readFlieSeek1() throws Exception { //1.创建配置文件信息 Configuration conf = new Configuration(); //2.获取文件系统 FileSystem fs = FileSystem.get(new URI(\u0026quot;hdfs://bigdata111:9000\u0026quot;), conf, \u0026quot;root\u0026quot;); //3.输入 Path path = new Path(\u0026quot;hdfs://bigdata111:9000/plus/hadoop-2.7.2.rar\u0026quot;); FSDataInputStream fis = fs.open(path); //4.输出 FileOutputStream fos = new FileOutputStream(\u0026quot;F:\\\\date\\\\readFileSeek\\\\A1\u0026quot;); //5.流对接 byte[] buf = new byte[1024]; for (int i = 0; i \u0026lt; 128 * 1024; i++) { fis.read(buf); fos.write(buf); } //6.关闭流 IOUtils.closeStream(fos); IOUtils.closeStream(fis); }  2）下载第二块\n/** * IO读取第二块的内容 * * @throws Exception */ @Test public void readFlieSeek2() throws Exception { //1.创建配置文件信息 Configuration conf = new Configuration(); //2.获取文件系统 FileSystem fs = FileSystem.get(new URI(\u0026quot;hdfs://bigdata111:9000\u0026quot;), conf, \u0026quot;root\u0026quot;); //3.输入 Path path = new Path(\u0026quot;hdfs://bigdata111:9000/plus/hadoop-2.7.2.rar\u0026quot;); FSDataInputStream fis = fs.open(path); //4.输出 FileOutputStream fos = new FileOutputStream(\u0026quot;F:\\\\date\\\\readFileSeek\\\\A2\u0026quot;); //5.定位偏移量/offset/游标/读取进度 (目的：找到第一块的尾巴，第二块的开头) fis.seek(128 * 1024 * 1024); //6.流对接 IOUtils.copyBytes(fis, fos, 1024); //7.关闭流 IOUtils.closeStream(fos); IOUtils.closeStream(fis); }  3）合并文件\n在window命令窗口中执行\ntype A2 \u0026raquo; A1 然后更改后缀为rar即可\n","id":8,"section":"posts","summary":"3.2 通过API操作HDFS 3.2.1 HDFS获取文件系统 1）详细代码 /** * 打印本地hadoop地址值 * IO的方式写代码 */ @Test public void intiHDFS() throws IOException { //F2 可以快速的定位错","tags":["hdfs"],"title":"HDFS基本操作","uri":"https://brightsails.github.io/2020/02/hdfs%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/","year":"2020"},{"content":"1.hadoop fs -put 命令出错 2.hadoop配置错误，需要有JAVA_HOME ","id":9,"section":"posts","summary":"1.hadoop fs -put 命令出错 2.hadoop配置错误，需要有JAVA_HOME","tags":null,"title":"Hadoop配置错误","uri":"https://brightsails.github.io/2020/02/hadoop_error/","year":"2020"},{"content":"一 HDFS概念 1.1 概念 HDFS，它是一个文件系统，**全称：Hadoop Distributed File System，用于存储文件通过目录树来定位文件；其次，它是分布式的，由很多服务器联合起来实现其功能，集群中的服务器有各自的角色。\n1.2 组成 1）HDFS集群包括，NameNode和DataNode以及Secondary Namenode。\n2）NameNode负责管理整个文件系统的元数据，以及每一个路径（文件）所对应的数据块信息。\n3）DataNode 负责管理用户的文件数据块，每一个数据块都可以在多个datanode上存储多个副本。\n4）Secondary NameNode用来监控HDFS状态的辅助后台程序，每隔一段时间获取HDFS元数据的快照。\n1.3 HDFS 文件块大小 HDFS中的文件在物理上是分块存储（block），块的大小可以通过配置参数( dfs.blocksize)来规定，默认大小在hadoop2.x版本中是128M，老版本中是64M\nHDFS的块比磁盘的块大，其目的是为了最小化寻址开销。如果块设置得足够大，从磁盘传输数据的时间会明显大于定位这个块开始位置所需的时间。因而，传输一个由多个块组成的文件的时间取决于磁盘传输速率。\n如果寻址时间约为10ms，而传输速率为100MB/s，为了使寻址时间仅占传输时间的1%，我们要将块大小设置约为100MB。默认的块大小128MB。\n块的大小：10ms100100M/s = 100M\n二 HFDS命令行操作 1）基本语法\nbin/hadoop fs 具体命令\n2）参数大全\n​ bin/hadoop fs\n[-appendToFile \u0026lt;localsrc\u0026gt; ... \u0026lt;dst\u0026gt;] [-cat [-ignoreCrc] \u0026lt;src\u0026gt; ...] [-checksum \u0026lt;src\u0026gt; ...] [-chgrp [-R] GROUP PATH...] [-chmod [-R] \u0026lt;MODE[,MODE]... | OCTALMODE\u0026gt; PATH...] [-chown [-R] [OWNER][:[GROUP]] PATH...] [-copyFromLocal [-f] [-p] \u0026lt;localsrc\u0026gt; ... \u0026lt;dst\u0026gt;] [-copyToLocal [-p] [-ignoreCrc] [-crc] \u0026lt;src\u0026gt; ... \u0026lt;localdst\u0026gt;] [-count [-q] \u0026lt;path\u0026gt; ...] [-cp [-f] [-p] \u0026lt;src\u0026gt; ... \u0026lt;dst\u0026gt;] [-createSnapshot \u0026lt;snapshotDir\u0026gt; [\u0026lt;snapshotName\u0026gt;]] [-deleteSnapshot \u0026lt;snapshotDir\u0026gt; \u0026lt;snapshotName\u0026gt;] [-df [-h] [\u0026lt;path\u0026gt; ...]] [-du [-s] [-h] \u0026lt;path\u0026gt; ...] [-expunge] [-get [-p] [-ignoreCrc] [-crc] \u0026lt;src\u0026gt; ... \u0026lt;localdst\u0026gt;] [-getfacl [-R] \u0026lt;path\u0026gt;] [-getmerge [-nl] \u0026lt;src\u0026gt; \u0026lt;localdst\u0026gt;] [-help [cmd ...]] [-ls [-d] [-h] [-R] [\u0026lt;path\u0026gt; ...]] [-mkdir [-p] \u0026lt;path\u0026gt; ...] [-moveFromLocal \u0026lt;localsrc\u0026gt; ... \u0026lt;dst\u0026gt;] [-moveToLocal \u0026lt;src\u0026gt; \u0026lt;localdst\u0026gt;] [-mv \u0026lt;src\u0026gt; ... \u0026lt;dst\u0026gt;] [-put [-f] [-p] \u0026lt;localsrc\u0026gt; ... \u0026lt;dst\u0026gt;] [-renameSnapshot \u0026lt;snapshotDir\u0026gt; \u0026lt;oldName\u0026gt; \u0026lt;newName\u0026gt;] [-rm [-f] [-r|-R] [-skipTrash] \u0026lt;src\u0026gt; ...] [-rmdir [--ignore-fail-on-non-empty] \u0026lt;dir\u0026gt; ...] [-setfacl [-R] [{-b|-k} {-m|-x \u0026lt;acl_spec\u0026gt;} \u0026lt;path\u0026gt;]|[--set \u0026lt;acl_spec\u0026gt; \u0026lt;path\u0026gt;]] [-setrep [-R] [-w] \u0026lt;rep\u0026gt; \u0026lt;path\u0026gt; ...] [-stat [format] \u0026lt;path\u0026gt; ...] [-tail [-f] \u0026lt;file\u0026gt;] [-test -[defsz] \u0026lt;path\u0026gt;] [-text [-ignoreCrc] \u0026lt;src\u0026gt; ...] [-touchz \u0026lt;path\u0026gt; ...] [-usage [cmd ...]]  3）常用命令实操\n（1）-help：输出这个命令参数\n​ bin/hdfs dfs -help rm\n（2）-ls: 显示目录信息\nhadoop fs -ls /\nHadoop fs -lsr /\n（3）-mkdir：在hdfs上创建目录\nhadoop fs -mkdir -p /hdfs路径\n（4）-moveFromLocal从本地剪切粘贴到hdfs\nhadoop fs -moveFromLocal 本地路径 /hdfs路径\n（5）\u0026ndash;appendToFile ：追加一个文件到已经存在的文件末尾\nhadoop fs -appendToFile 本地路径 /hdfs路径\n（6）-cat ：显示文件内容\n​ hadoop fs -cat /hdfs路径\n（7）-tail -f：监控文件\nhadoop fs -tail -f /hdfs路径\n（8）-chmod、-chown：linux文件系统中的用法一样，修改文件所属权限\nhadoop fs -chmod 777 /hdfs路径\nhadoop fs -chown someuser:somegrp /hdfs路径\n（9）-cp ：从hdfs的一个路径拷贝到hdfs的另一个路径\nhadoop fs -cp /hdfs路径1 / hdfs路径2\n（10）-mv：在hdfs目录中移动/重命名 文件\nhadoop fs -mv /hdfs路径 / hdfs路径\n（11）-get：等同于copyToLocal，就是从hdfs下载文件到本地\nhadoop fs -get / hdfs路径 ./本地路径\n（12）-getmerge ：合并下载多个文到linux本地，比如hdfs的目录 /aaa/下有多个文件:log.1, log.2,log.3,\u0026hellip;（注：是合成到Linux本地）\nhadoop fs -getmerge /aaa/log.* ./log.sum\n合成到不同的目录：hadoop fs -getmerge /hdfs1路径 /hdfs2路径 /\n（13）-put：等同于copyFromLocal\nhadoop fs -put /本地路径 /hdfs路径\n（14）-rm：删除文件或文件夹\nhadoop fs -rm -r /hdfs路径\n（15）-df ：统计文件系统的可用空间信息\nhadoop fs -df -h / hdfs路径\n（16）-du统计文件夹的大小信息\n[itstar@bigdata111 hadoop-2.8.4]$ hadoop fs -du -s -h / hdfs路径\n188.5 M /user/itstar/wcinput\n[itstar@bigdata111 hadoop-2.8.4]$ hadoop fs -du -h / hdfs路径\n188.5 M / hdfs路径\n97 / hdfs路径\n（17）-count：统计一个指定目录下的文件节点数量\nhadoop fs -count /aaa/\n[itstar@bigdata111 hadoop-2.8.4]$ hadoop fs -count / hdfs路径\n​ 1 2 197657784 / hdfs路径\n嵌套文件层级； 包含文件的总数\n（18）-setrep：设置hdfs中文件的副本数量：3是副本数，可改\nhadoop fs -setrep 3 / hdfs路径\n![img]这里设置的副本数只是记录在namenode的元数据中，是否真的会有这么多副本，还得看datanode的数量。因为目前只有3台设备，最多也就3个副本，只有节点数的增加到10台时，副本数才能达到10。\n3.1 IDEA环境准备 {$MAVEN_HOME/conf/settings}\n \u0026lt;!--本地仓库所在位置--\u0026gt; \u0026lt;localRepository\u0026gt;F:\\m2\\repository\u0026lt;/localRepository\u0026gt; \u0026lt;!--使用阿里云镜像去下载Jar包，速度更快--\u0026gt; \u0026lt;mirrors\u0026gt; \u0026lt;mirror\u0026gt; \u0026lt;id\u0026gt;alimaven\u0026lt;/id\u0026gt; \u0026lt;name\u0026gt;aliyun maven\u0026lt;/name\u0026gt; \u0026lt;url\u0026gt;http://maven.aliyun.com/nexus/content/groups/public/\u0026lt;/url\u0026gt; \u0026lt;mirrorOf\u0026gt;central\u0026lt;/mirrorOf\u0026gt; \u0026lt;/mirror\u0026gt; \u0026lt;/mirrors\u0026gt;  3.1.0 Maven配置 3.1.1 Maven准备 \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.hadoop\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;hadoop-common\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.8.4\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.hadoop\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;hadoop-hdfs\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.8.4\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.hadoop\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;hadoop-client\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.8.4\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.projectlombok\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;lombok\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.16.10\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;log4j\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;log4j\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.2.17\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.slf4j\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;slf4j-api\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.7.7\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- https://mvnrepository.com/artifact/junit/junit --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;junit\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;junit\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;4.12\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt;  3.1.2 IDEA准备 1）配置HADOOP_HOME环境变量\n2）采用hadoop编译后的bin 、lib两个文件夹（如果不生效，重新启动IDEA）\n3）创建第一个java工程\npublic class HdfsClientDemo1 { public static void main(String[] args) throws Exception { // 1 获取文件系统 Configuration configuration = new Configuration(); // 配置在集群上运行 configuration.set(\u0026quot;fs.defaultFS\u0026quot;, \u0026quot;hdfs://bigdata111:9000\u0026quot;); FileSystem fileSystem = FileSystem.get(configuration); // 直接配置访问集群的路径和访问集群的用户名称 //\tFileSystem fileSystem = FileSystem.get(new URI(\u0026quot;hdfs://bigdata111:9000\u0026quot;),configuration, \u0026quot;itstar\u0026quot;); // 2 把本地文件上传到文件系统中 fileSystem.copyFromLocalFile(new Path(\u0026quot;f:/hello.txt\u0026quot;), new Path(\u0026quot;/hello1.copy.txt\u0026quot;)); // 3 关闭资源 fileSystem.close(); System.out.println(\u0026quot;over\u0026quot;); } }  4）执行程序\n注：eclipse运行时可能需要配置用户名称\n客户端去操作hdfs时，是有一个用户身份的。默认情况下，hdfs客户端api会从jvm中获取一个参数来作为自己的用户身份：-DHADOOP_USER_NAME=itstar，itstar为用户名称。\n","id":10,"section":"posts","summary":"一 HDFS概念 1.1 概念 HDFS，它是一个文件系统，**全称：Hadoop Distributed File System，用于存储文件通过目录树来定位文件；其次，它是分布式的","tags":["hdfs"],"title":"HDFS概念","uri":"https://brightsails.github.io/2020/02/hdfs%E6%A6%82%E5%BF%B5/","year":"2020"},{"content":" 1、ssh  SSH 为 Secure Shell 的缩写，SSH 为建立在应用层基础上的安全协议。SSH 是目前较可靠，专为远程登录会话和其他网络服务提供安全性的协议。利用 SSH 协议可以有效防止远程管理过程中的信息泄露问题。\n 2、ssh-keygen  从客户端来看，SSH提供两种级别的安全验证：\n​ 第一种级别（基于口令的安全验证）：只要你知道自己帐号和口令，就可以登录到远程主机。所有传输的数据都会被加密，但是不能保证你正在连接的服务器就是你想连接的服务器。可能会有别的服务器在冒充真正的服务器，也就是受到“中间人”这种方式的攻击。\n​ 第二种级别（基于密钥的安全验证）ssh-keygen：需要依靠密钥，这里的密钥是非对称密钥。\n 3、t : t是type的缩写  ​ -t 即指定密钥的类型，密钥的类型有两种，一种是RSA，一种是DSA\n 4、rsa：是指RSA算法  ​ RSA：RSA加密算法是一种非对称加密算法，是由三个麻省理工的牛人弄出来的，RSA是他们三个人姓的开头首字母组合。\n​ DSA：Digital Signature Algorithm (DSA)是Schnorr和ElGamal签名算法的变种。\n为了让两个linux机器之间使用ssh不需要用户名和密码。所以采用了数字签名RSA或者DSA来完成这个操作。ssh-keygen默认使用rsa密钥，所以不加-t rsa也行，如果你想生成dsa密钥，就需要加参数-t dsa。\n 5、b ：b是bit的缩写  ​ -b 指定密钥长度。\n  6、4096\n对于RSA密钥，最小要求768位，默认是2048位。4096指的是RSA密钥长度为4096位。\nDSA密钥必须恰好是1024位(FIPS 186-2 标准的要求)。\n  7、C：C是comment的缩写\n  ​ -C表示提供一个注释，用于识别这个密钥。\nubuntu install ssh 1. sudo apt install openssh-client #本地主机运行此条，实际上通常是默认安装client端程序的 sudo apt install openssh-server #服务器运行此条命令安装 2. sudo /etc/init.d/ssh stop #server停止ssh服务 sudo /etc/init.d/ssh restart #server重启ssh服务 3. 查看并启动SSH:sudo ps -e | grep ssh 4.查看或修改SSH配置文件 如果有PermitRootLogin without-password,加一个”#”号，注释掉该行，并增加一句PermitRootLogin yes。 4. ifconfig #查询ip地址，在返回信息中找到自己的ip地址 sudo vi /etc/network/interfaces修改ip sudo /etc/init.d/networking restart 重启网络服务 5.查看ssh状态 service ssh status  \t如果出现”无法打开锁 “类似的提示可以重启系统，切换到 root 用户再次执行,也可以直接切换到 root 用户尝试。 如果还是链接不上请用root权限在终端输入ufw status查询防火墙是否开启，如开启输入：ufw disable 命令关闭防火墙即可链接。  ubuntu不能链接问题 ​\t今天开启后，ssh莫名不能工作了，在经过一番勘察之后，发现防火墙出现问题，虽然防火墙状态是不工作，但是依然不能使用。原因可能是Ubuntu中默认关闭了SSH 服务，即没有启用22号端口。\n​\tubuntu没有开启22号端口是不能连接上SSH或者XShell等软件，需要打开SSH服务，开启22端口，执行以下命令：\n可以使用以下两种方法查看是否开启了22号端口：(发现端口此时的状态都为LISTEN，即为开启状态） netstat -ntlp|grep 22 lsof -i:22  ","id":11,"section":"posts","summary":"1、ssh SSH 为 Secure Shell 的缩写，SSH 为建立在应用层基础上的安全协议。SSH 是目前较可靠，专为远程登录会话和其他网络服务提供安全性的协议。利用 SSH 协","tags":["环境搭建"],"title":"SSH","uri":"https://brightsails.github.io/2020/02/ssh/","year":"2020"},{"content":"大数据的特征   容量（Volume）：数据的大小决定所考虑的数据的价值和潜在的信息；\n  种类（Variety）：数据类型的多样性；\n  速度（Velocity）：指获得数据的速度；\n  可变性（Variability）：妨碍了处理和有效地管理数据的过程。\n  真实性（Veracity）：数据的质量\n  复杂性（Complexity）：数据量巨大，来源多渠道\n  价值（value）：合理运用大数据，以低成本创造高价值\n  Hadoop生态   HDFS =====\u0026gt; 解决存储问题\n  MapReduce =====\u0026gt; 解决计算问题\n  Yarn =====\u0026gt; 资源协调者\n  Zookeeper =====\u0026gt; 分布式应用程序协调服务\n  Flume =====\u0026gt; 日志收集系统\n  Hive =====\u0026gt; 基于Hadoop的数仓工具\n  HBase =====\u0026gt; 分布式、面向列的开源数据库\n  Sqoop =====\u0026gt; 数据传递工具\n  Scala =====\u0026gt; 多范式编程语言、面向对象和函数式编程的特性\n  Spark =====\u0026gt; 目前企业常用的批处理离线/实时计算引擎\n  Flink =====\u0026gt; 目前最火的流处理框架、既支持流处理、也支持批处理\n  Elasticsearch =====\u0026gt; 大数据分布式弹性搜索引擎\n  Docker =====\u0026gt;Docker 是一个开源的应用容器\n  Hadoop的优势  高可靠性：  因为Hadoop假设计算元素和存储会出现故障，因为它维护多个工作数据副本，在出现故障时可以对失败的节点重新分布处理。\n 高扩展性：  在集群间分配任务数据，可方便的扩展数以千计的节点。\n 高效性：  在MapReduce的思想下，Hadoop是并行工作的，以加快任务处理速度。\n 高容错性：  自动保存多份副本数据，并且能够自动将失败的任务重新分配。\n  Hadoop****组成\n  Hadoop HDFS：\n  一个高可靠、高吞吐量的分布式文件系统。\n  Hadoop MapReduce：\n  一个分布式的离线并行计算框架。\n  Hadoop YARN：\n  作业调度与集群资源管理的框架。\n  Hadoop Common：\n  支持其他模块的工具模块（Configuration、RPC、序列化机制、日志操作）。\n  HDFS****架构概述\n  Namenode**：存储****元数据**\n  Datanode**：存储数据的节点，会对数据块进行校验**\n  Secondarynamenode**：监控namenode 的元数据，****每隔一定的时间进行元数据的合并**\n  YARN****架构概述\n  ResourceManager(rm)****：\n  处理客户端请求、启动/监控ApplicationMaster、监控NodeManager、资源分配与调度\n NodeManager(nm)****：  单个节点上的资源管理、处理来自ResourceManager的命令、处理来自ApplicationMaster的命令\n ApplicationMaster**：**  数据切分、为应用程序申请资源，并分配给内部任务、任务监控与容错\n Container**：**  对任务运行环境的抽象，封装了CPU、内存等多维资源以及环境变量、启动命令等任务运行相关的信息\n  MapReduce****架构概述\n  MapReduce将计算过程分为两个阶段：Map和Reduce\n  Map阶段并行处理输入数据\n  Reduce阶段对Map结果进行汇总\n  三 Hadoop运行环境搭建\n环境配置\n  关闭防火墙\n  关闭防火墙：systemctl stop firewalld.service\n  禁用防火墙：systemctl disable firewalld.service\n  查看防火墙：systemctl status firewalld.service\n  (学习阶段关闭防火墙)\n  关闭Selinux：vi /etc/selinux /config\n  将SELINUX=enforcing改为SELINUX=disabled\n  修改IP\n  善用Tab键\n  vi /etc/sysconfig/network-scripts/eno16777736\n  BOOTPROTO=static\n  ONBOOT=yes\n   IPADDR=192.168.X.51\n  GATEWAY=192.168.X.2\n  DNS1=8.8.8.8\n  DNS2=8.8.4.4\n  NETMASK=255.255.255.0\n  vi /etc/resolv.conf\n  nameserver 8.8.8.8\n  nameserver 8.8.4.4\n  重启网卡：servie network restart\n  修改主机名\n  hostnamectl set-hostname 主机名\n  IP****和主机名关系映射\n  vi /etc/hosts\n  192.168.1.121 bigdata111\n192.168.1.122 bigdata112\n192.168.1.123 bigdata113\n 在windows的C:\\Windows\\System32\\drivers\\etc路径下找到hosts并添加  192.168.1.121 bigdata111\n192.168.1.122 bigdata112\n192.168.1.123 bigdata113\n修改主机名：\nvi /etc/hostname\nbigdata111\n Xshell  输入IP、用户名和密码\n  在opt目录下创建文件（此步可选）\n  创建itstar用户\n  adduser itstar\n  passwd itstar\n  设置itstar用户具有root权限\n  vi /etc/sudoers 92行 找到root ALL=(ALL) ALL\n  复制一行：itstar ALL=(ALL) ALL\n  安装jdk\n  卸载现有jdk\n  （1）查询是否安装java软件：\nrpm -qa|grep java\n（2）如果安装的版本低于1.7，卸载该jdk：\nrpm -e 软件包名字\n 在/opt目录下创建两个子文件  mkdir /opt/mod /opt/soft\n 解压jdk到/opt/module目录下  tar -zxvf jdk-8u144-linux-x64.tar.gz -C /opt/mod/\n 配置jdk环境变量  vi /etc/profile\nexport JAVA_HOME=/opt/mod/jdk1.8.0_144 export PATH=$PATH:$JAVA_HOME/bin:$PATH :为分隔符 后面的PATH为系统的path路径 建议$PATH写前面，若自己的path写错，不影响系统PATH source /etc/profile 应用profile    测试jdk安装成功\n  java -version\n  java version \u0026ldquo;1.8.0_144\u0026rdquo;\n  四 Hadoop运行模式\n伪/完全分布式部署Hadoop\n  SSH****无密码登录\n  生成公钥和私钥：ssh-keygen -t rsa\n  然后敲（三个回车），就会生成两个文件id_rsa（私钥）、id_rsa.pub（公钥）\n  将公钥拷贝到要免密登录的目标机器上\n  ssh-copy-id 主机名1\n  ssh-copy-id 主机名2\n  ssh-copy-id 主机名3\n  注：在另外两台机器上分别执行，共执行9遍\n .ssh****文件夹下的文件功能解释  （1）~/.ssh/known_hosts ：记录ssh访问过计算机的公钥(public key)\n（2）id_rsa ：生成的私钥\n（3）id_rsa.pub ：生成的公钥\n（4）authorized_keys ：存放授权过得无秘登录服务器公钥\n 配置集群(表格版)   集群部署规划:      bigdata111 bigdata112 bigdata113     HDFS NameNode SecondaryNameNode DataNode DataNode DataNode   YARN NodeManager ResourceManager NodeManager NodeManager     配置文件：     文件 配置     core-site.xml fs.defaultFShdfs://主机名1:9000hadoop.tmp.dir/opt/module/hadoop-2.X.X/data/tmp   hdfs-site.xml dfs.replication3dfs.namenode.secondary.http-address主机名1:50090dfs.permissionsfalse   yarn-site.xml yarn.nodemanager.aux-servicesmapreduce_shuffleyarn.resourcemanager.hostname主机名1yarn.log-aggregation-enabletrueyarn.log-aggregation.retain-seconds604800   mapred-site.xml mapreduce.framework.nameyarnmapreduce.jobhistory.address主机名1:10020mapreduce.jobhistory.webapp.address主机名1:19888   hadoop-env.sh、yarn-env.sh、mapred-env.sh（分别在这些的文件中添加下面的路径） export JAVA_HOME=/opt/module/jdk1.8.0_144（注：是自己安装的路径）    slaves bigdata111、bigdata112、bigdata113（自己设置的主机名）     格式化Namenode：  hdfs namenode -format\n为什么要格式化？\nNameNode主要被用来管理整个分布式文件系统的命名空间(实际上就是目录和文件)的元数据信息，同时为了保证数据的可靠性，还加入了操作日志，所以，NameNode会持久化这些数据(保存到本地的文件系统中)。对于第一次使用HDFS，在启动NameNode时，需要先执行-format命令，然后才能正常启动NameNode节点的服务。\n格式化做了哪些事情？\n在NameNode节点上，有两个最重要的路径，分别被用来存储元数据信息和操作日志，而这两个路径来自于配置文件，它们对应的属性分别是dfs.name.dir和dfs.name.edits.dir，同时，它们默认的路径均是/tmp/hadoop/dfs/name。格式化时，NameNode会清空两个目录下的所有文件，之后，会在目录dfs.name.dir下创建文件\nhadoop.tmp.dir 这个配置，会让dfs.name.dir和dfs.name.edits.dir会让两个目录的文件生成在一个目录里\n 启动集群得命令：  namenode和ResourceManger在一台机器上：start-all.sh\nNamenode的主节点：sbin/start-dfs.sh\nYarn的主节点：sbin/start-yarn.sh\n注意：Namenode和ResourceManger如果不是同一台机器，不能在NameNode上启动 yarn，应该在ResouceManager所在的机器上启动yarn。\n scp****文件传输  实现两台远程机器之间的文件传输（bigdata112主机文件拷贝到bigdata113主机上）\nscp -r [文件] 用户@主机名：绝对路径\n注：伪分布式是一台、完全分布是三台\n 完全分布式  步骤：\n1）克隆2台客户机（关闭防火墙、静态ip、主机名称）\n2）安装jdk\n3）配置环境变量\n4）安装hadoop\n5）配置环境变量\nexport JAVA_HOME=/opt/mod/jdk1.8.0_144\nexport HADOOP_HOME=/opt/mod/hadoop-2.8.4\nexport PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME\\bin:$HADOOP_HOME\\sbin\n6）安装ssh\n7）配置集群\n8）启动测试集群\n注：此配置直接使用虚拟机克隆伪分布式两台即可\n  自带官方wordcount案例\n  随意上传一个文本文件\n  **上传命令：**hadoop fs -put 文件名 /\n  执行命令：\n  hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.X.X.jar wordcount /入 /出\n 命令解析：  hadoop jar 路径的jar包 全类名 输入路径 输出路径\n 查看结果：  hadoop fs -cat 路径\nHadoop****启动和停止命令：\n以下命令都在$HADOOP_HOME/sbin下，如果直接使用，记得配置环境变量\n   启动/停止历史服务器 mr-jobhistory-daemon.sh start|stop historyserver     启动/停止总资源管理器 yarn-daemon.sh start|stop resourcemanager   启动/停止节点管理器 yarn-daemon.sh start|stop nodemanager   启动/停止 NN 和 DN start|stop-dfs.sh   启动/停止 RN 和 NM start|stop-yarn.sh   启动/停止 NN、DN、RN、NM start|stop-all.sh   启动/停止 NN hadoop-daemon.sh start|stop namenode   启动/停止 DN hadoop-daemon.sh start|stop datanode    ","id":12,"section":"posts","summary":"大数据的特征 容量（Volume）：数据的大小决定所考虑的数据的价值和潜在的信息； 种类（Variety）：数据类型的多样性； 速度（Veloci","tags":["hadoop"],"title":"hadoop配置","uri":"https://brightsails.github.io/2020/02/hadoop%E9%85%8D%E7%BD%AE/","year":"2020"},{"content":"Linux中Makefile设计 1.编译流程 1.预编译 gcc -E hello.c -o hello.i 目的： 1.展开宏定义 2.引入头文件，将.h引入.c 2.汇编 gcc -S hello.i -o hello.s 目的： 将C语言转换为汇编代码 3.编译 gcc -c hello.s -o hello.o 4.链接 gcc -o hello hello.o -S 汇编 -o 输出 -c 编译 linux对文件后缀没有判断  使用MakeFile命令默认在当前目录下寻找文件名为MakeFile的文件\n2.设计makefile 显示规则 \t//要对生成的目标所添加的依赖 格式： target:dep cmd //makefile先检测依赖关系是否存在，若存在则执行代码，否则跳过，查询其他指令是否能达成依赖条件 hello.i:hello.c  变量 OBJ = OBJ := OBJ +=\n头对象 TARGET=要生成的文件名 调用： $(文件名)\n尾对象： .PHONY: rm -rf $(变量名) 调用方法： make clean\n通配符  % 任意一个 ？ 所有 *所有 $@ 代表目标文件 $^ 代表依赖文件 $\u0026lt; 代表第一个依赖文件  Makefile关键字参数(自定义变量) TARGET = 最后要生成的文件名 INCLUDE= 头文件 DEFS = 宏定义的参数 CFLAGES =gg编译参数 CC=gcc LIBS = 库 -l -L SRC(wildcard *.c） 查找返回目录下所有.c文件 OBJ =$(patsubst %.c,%.o $(SRC))\npatsubst 函数是将SRC列表中的.c文件转换为.o存进obj，对文件没有任何影响。  符号表 1.readelf -h filename查看头 2.readelf -s filename 查看头文件的符号表 3.objcopy —only-keep-debug filename filename.symbol.1.0(版本) 剥离出readelf（符号表），原文件不受影响 4.objcopy strip-debug filename filename.release 剥离符号表 *（拓展） stripe filename.release 剥离符号表（深度清除符号表）  基本编译命令 多数UNIX平台都通过CC调用它们的C编译程序.除标准和CC以外,LINUX和FREEBSD还支持gcc，基本的编译命令有以下几种:\n1. -c 编译产生对象文件(*.obj)而不链接成可执行文件,当编译几个独立的模块,而待以后由链接程序把它们链接在一起时,就可以使用这个选项,如: $cc -c hello.c ===\u0026gt; hello.o $cc hello.o 2. -o 允许用户指定输出文件名,如 $cc hello.c -o hello.o or $cc hello.c -o hello 3.-g 指明编译程序在编译的输出中应产生调试信息.这个调试信息使源代码和变量名引用在调试程序中或者当程序异常退出后在分析core文件时可被使用. $cc -c -g hello.c 4. -D 允许从编译程序命令行定义宏符号 一共有两种情况:一种是用-DMACRO,相当于在程序中使用#define MACRO,另一种是用-DMACRO=A,相当于程序中的#define MACRO A.如对下面这代码: #ifdefine DEBUG printf(\u0026quot;debug message\\n\u0026quot;); #endif 编译时可加上-DDEBUG=1参数,执行程序则打印出编译信息 gcc -o -DDEBUG=1 -DCHANDLKE -DTANZHOU printf printf.c 5. -I 可指定查找include文件的其他位置.例如,如果有些include文件位于比较特殊的地方,比如/usr/local/include,就可以增加此选项如下: $cc -c -I/usr/local/include -I/opt/include hello.c 此时目录搜索会按给出的次序进行. 6. -E 这个选项是相对标准的,它允许修改命令行以使编译程序把预先处理的C文件发到标准输出,而不实际编译代码. 在查看C预处理伪指令和C宏时,这是很有用的.可能的编译输出可重新定向到一个文件,然后用编辑程序来分析: $cc -c -E hello.c \u0026gt;cpp.out\t此命令使include文件和程序被预先处理并重定向到文cpp.out。 以后可以用编辑程序或者分页命令分析这个文件,并确定最终的C语言代码看起来如何. 7.\t-o 优化选项,这个选项不是标准的 -O和 -O1指定1级优化 -O2 指定2级优化 -O3 指定3级优化 -O0指定不优化 $cc -c O3 -O0 hello.c 当出现多个优化时,以最后一个为准!! 8.-Wall 以最高级别使用GNU编译程序,专门用于显示警告用!! $gcc -Wall hello.c 9.-L指定连接库的搜索目录,-l(小写L)指定连接库的名字 $gcc main.o -L/usr/lib -lqt -lpthread -lrt -o hello 上面的命令把目标文件main.o与库qt相连接,连接时会到/usr/lib查找这个库文件.也就是说-L与-l一般要成对出现.  成品 TARGET=mult_proxy //最终目标文件的名字 SRC = $(wildcard *.c) //遍历当前目录下的全部.c文件，形成一个列表 OBJ = $(patsubst %.c,%.o,$(SRC)) //将变量src下面的全部.c结尾的文件，转换为.o存入OBJ INCLUDE = DEFS = CFLAGS = -g CC =gcc LIBS = RELEASE=release SYMBOL=debug $(TARGET):$(OBJ) $(CC) $(CFLAGS) $(DEFS) -o $@ $^ $(LIBS) make看到一个[.o]文件，它就会自动的把[.c]文件加在依赖关系中，如果make找到一个whatever.o，那么whatever.c，就会是whatever.o的依赖文件。 1. gcc -g -o mult_proxy $(OBJ) 2.obj中是.o文件，则去寻找.c件 3.返回至1 $(RELEASE):$(TARGET) objcopy --strip-debug $(TARGET) $(TARGET).release strip $(TARGET).release $(SYMBOL):$(TARGET) objcopy --only-keep-debug $(TARGET) $(TARGET).symbol .PHONY: .PHONY意思表示clean是一个“伪目标”，不成文的规矩是——“clean从来都是放在文件的最后”。 clean: rm -rf *.o $(TARGET) $(TARGET).release $(TARGET).symbol  程序去符号表步骤 确认程序是否存在符号表 readelf -s test-1 生成符号表 objcopy --only-keep-debug filename filename.symbol 生成发布程序 objcopy --strip-debug filename filename.relase 使用符号表进行debug gdb -q --symbol=filename.symbol --exec=filename-release symbol-file .filename.symbol  patsubst 函数 1、wildcard : 扩展通配符\n2、notdir ： 去除路径\n3、patsubst ：替换通配符\n例子： 建立一个测试目录，在测试目录下建立一个名为sub的子目录 $ mkdir test $ cd test $ mkdir sub\n在test下，建立a.c和b.c2个文件，在sub目录下，建立sa.c和sb.c2 个文件\n建立一个简单的Makefile src=$(wildcard .c ./sub/.c) dir=$(notdir $(src)) obj=$(patsubst %.c,%.o,$(dir) )\nall: @echo $(src) @echo $(dir) @echo $(obj) @echo \u0026ldquo;end\u0026rdquo;\n执行结果分析： 第一行输出： a.c b.c ./sub/sa.c ./sub/sb.c\nwildcard把 指定目录 ./ 和 ./sub/ 下的所有后缀是c的文件全部展开。\n第二行输出： a.c b.c sa.c sb.c notdir把展开的文件去除掉路径信息\n第三行输出： a.o b.o sa.o sb.o\n在$(patsubst %.c,%.o,$(dir) )中，patsubst把$(dir)中的变量符合后缀是.c的全部替换成.o， 任何输出。 或者可以使用 obj=$(dir:%.c=%.o) 效果也是一样的。\n这里用到makefile里的替换引用规则，即用您指定的变量替换另一个变量。 它的标准格式是 $(var:a=b) 或 ${var:a=b} 它的含义是把变量var中的每一个值结尾用b替换掉a\n今天在研究makefile时在网上看到一篇文章，介绍了使用函数wildcard得到指定目录下所有的C语言源程序文件名的方法，这下好了，不用手工一个一个指定需要编译的.c文件了，方法如下：\nSRC = $(wildcard *.c)\n等于指定编译当前目录下所有.c文件，如果还有子目录，比如子目录为inc，则再增加一个wildcard函数，象这样：\nSRC = $(wildcard .c) $(wildcard inc/.c)\n也可以指定汇编源程序： ASRC = $(wildcard *.S)\n这样一来，makefile模板可修改的基本就是AVR名称和时钟频率了，其它的一般不用动了。\nPS：针对patsubst我们来好好聊一聊\n这是个模式替换函数\n格式：$(patsubst ,,) 名称：模式字符串替换函数——patsubst。 功能：查找中的单词（单词以“空格”、“Tab”或“回车”“换行”分隔）是否符合模式，如果匹配的话，则以替换。这里，可以包括通配符“%”，表示任意长度的字串。如果中也包含“%”，那么，中的这个“%”将是中的那个“%”所代表的字串。（可以用“\\”来转义，以“%”来表示真实含义的“%”字符） 返回：函数返回被替换过后的字符串。\n示例：\n$(patsubst %.c,%.o,x.c.c bar.c)\n把字串“x.c.c bar.c”符合模式[%.c]的单词替换成[%.o]，返回结果是“x.c.o bar.o”\nmake中有个变量替换引用\n对于一个已经定义的变量，可以使用“替换引用”将其值中的后缀字符（串）使用指定的字符（字符串）替换。格式为“$(VAR:A=B)”（或者“${VAR:A=B}”），意思是，替换变量“VAR”中所有“A”字符结尾的字为“B”结尾的字。“结尾”的含义是空格之前（变量值多个字之间使用空格分开）。而对于变量其它部分的“A”字符不进行替换。例如：\nfoo := a.o b.o c.o\nbar := $(foo:.o=.c)\n在这个定义中，变量“bar”的值就为“a.c b.c c.c”。使用变量的替换引用将变量“foo”以空格分开的值中的所有的字的尾字符“o”替换为“c”，其他部分不变。如果在变量“foo”中如果存在“o.o”时，那么变量“bar”的值为“a.c b.c c.c o.c”而不是“a.c b.c c.c c.c”。\n它是patsubst的一个简化，那么到底是简化成了什么样子呢\nCROSS=\nCC=$(CROSS)gcc\nCFLAGS= -Wall\nLDFLAGS=\nPKG = src\nSRCS = $(wildcard $(PKG)/inc/.c) $(wildcard $(PKG)/.c)\nBOJS = $(patsubst %.c,%.o,$(SRCS))\n#BOJS = $(SRCS: .c = .o)\n#%.o:%.c\n# $(CC) -c $\u0026lt; $(CFLAGS) -o $@\n.PHONY:main\nmain:$(BOJS)\n-$(CC) -o $@ $(CFLAGS) $^ $(LDFLAGS)\n-mv main ./myfile\n起初使用的是变量替换引用的方式，但是却始终不生成中间的.o文件，但是使用patsubst后，一切正常了，如果你知道为什么，请留言告诉我吧\n","id":13,"section":"posts","summary":"Linux中Makefile设计 1.编译流程 1.预编译 gcc -E hello.c -o hello.i 目的： 1.展开宏定义 2.引入头文件，将.h引入.c 2.汇编 gcc -S hello.i -o hello.s 目的： 将","tags":["makefile"],"title":"Makefile设计","uri":"https://brightsails.github.io/2020/02/makefile/","year":"2020"},{"content":"linux文件权限管理  共10位 r 读 w 写 x 执行 第1位为：d目录 -文件 l软连接代码 第2-5位为：文件所属者权限 第5-8位为：所属组 第8-10位为：其他用户  Linux查看命令  ls -l ==ll 查看该目录下文件的详细信息 ls -a 查看该目录下所有文件（隐藏文件） la -ls = ll+la ls -lhi 显示存储号 touch 创建文件 mv a b 将a改名为b chmod ： 文件所有者 u 同组 g 其他用户 o  chmod ug+r 文件名 chmod 777 +文件名 三位的二进制 文件所有者与同组成员增加读的权限    chown [最终用户] [文件或目录] 功能描述：改变文件或者目录的所有者\n[root@bigdata111 test1]# chown itstar test1.java [root@bigdata111 test1]# ls -al -rwxr-xr-x. 1 itstar itstar 551 5月 23 13:02 test1.java 修改前： [root@bigdata111 xiyou]# ll drwxrwxrwx. 2 root root 4096 9月 3 21:20 sunhouzi 修改后 [root@bigdata111 xiyou]# chown -R itstar:itstar sunhouzi/ [root@bigdata111 xiyou]# ll drwxrwxrwx. 2 itstar itstar 4096 9月 3 21:20 sunhouzi    vi基础操作  yy 复制一行 yNy 复制N行 p 粘贴 u 撤销 dd 删除一行 dNd 删除N行 shift+6 光标移至行头 shift+7 光标移至行尾 shift+g 光标移至最后一行 set nu 显示行号 set nonu 关闭行号 N shift+g 移至N行  Linux文件目录  / 根目录\t/root ～ 家目录 /root/username ctrl +l ，clear 清屏 mkdir 文件名 创建目录 mkdir -p aa/aa1/aa2 创建多级目录 touch 创建空文件 cp 文件名 指定目录 复制文件至指定目录 cp -r 目录名 指定目录 复制目录至指定目录 rm 文件名 删除文件（有提示，不能删除目录） rm -fr——删除文件（无提示） mv 文件名 指定目录——移动文件 cat 文件名 查看文件 more 文件名 查看文件分页 —\u0026gt;按空格——向下移动 —\u0026gt;ctrl + b——向上移动 tail -F 文件名——监控文件 echo 追加内容 \u0026raquo;文件名 \u0026ndash;\u0026gt;文件末追加内容（用于日志追踪） ctrl +c 退出监控 ln -s[源文件][目标文件] 软连接 history 历史命令  时间日期命令   data 显示当前时间\n  data -s 设置系统时间\n  cal -m 查看m月的日历\n  data +%Y 显示年份。\n（1）date -d '1 days ago'\t（功能描述：显示前一天日期） （2）date -d yesterday +%Y%m%d\t（同上） （3）date -d next-day +%Y%m%d\t（功能描述：显示明天日期） （4）date -d 'next monday'\t（功能描述：显示下周一时间）    用户管理命令   useradd 用户名 添加用户\n  passwd 密码 设置密码（root用户下）\n  userdel 删除用户\n  id 用户名 查看用户是否存在\n  su 切换用户\n  vi /etc/sudoers 修改用户权限\n  groupadd 新增用户组\n  cat etc/group 查看组\n  chown [最终用户][文件或目录]\n  chown -R 修改用户所属目录和所属组\n  who 查看登录用户信息\n（1）whoami\t（功能描述：显示自身用户名称） （2）who am i\t（功能描述：显示登录用户的用户名） （3）who\t（功能描述：看当前有哪些用户登录到了本台机器上）    设置itstar普通用户具有root权限\n修改 /etc/sudoers 文件，找到下面一行，在root下面添加一行，如下所示： ## Allow root to run any commands anywhere root ALL=(ALL) ALL itstar ALL=(ALL) ALL 或者配置成采用sudo命令时，不需要输入密码 ## Allow root to run any commands anywhere root ALL=(ALL) ALL itstar ALL=(ALL) NOPASSWD:ALL 修改完毕，现在可以用itstar帐号登录，然后用命令 su - ，即可获得root权限进行操作。    1.7 磁盘分区类 1.1.1 fdisk查看分区 1）基本语法：\n​ fdisk -l （功能描述：查看磁盘分区详情）\n​ 注意：在root用户下才能使用\n2）功能说明：\n​ （1）Linux分区\n这个硬盘是20G的，有255个磁面；63个扇区；2610个磁柱；每个 cylinder（磁柱）的容量是 8225280 bytes=8225.280 K（约为）=2.225280M（约为）；\n   Device Boot Start End Blocks Id System     分区序列 引导 从X磁柱开始 到Y磁柱结束 容量 分区类型ID 分区类型    （2）Win7分区\n3）案例\n[root@bigdata111 /]# fdisk -l\nDisk /dev/sda: 21.5 GB, 21474836480 bytes\n255 heads, 63 sectors/track, 2610 cylinders\nUnits = cylinders of 16065 * 512 = 8225280 bytes\nSector size (logical/physical): 512 bytes / 512 bytes\nI/O size (minimum/optimal): 512 bytes / 512 bytes\nDisk identifier: 0x0005e654\nDevice Boot Start End Blocks Id System\n/dev/sda1 * 1 26 204800 83 Linux\nPartition 1 does not end on cylinder boundary.\n/dev/sda2 26 1332 10485760 83 Linux\n/dev/sda3 1332 1593 2097152 82 Linux swap / Solaris\n1.1.2 df查看硬盘 1）基本语法：\n​ df 参数 （功能描述：列出文件系统的整体磁盘使用量，检查文件系统的磁盘空间占用情况）\n参数：\n-a ：列出所有的文件系统，包括系统特有的 /proc 等文件系统；\n-k ：以 KBytes 的容量显示各文件系统；\n-m ：以 MBytes 的容量显示各文件系统；\n-h ：以人们较易阅读的 GBytes, MBytes, KBytes 等格式自行显示；\n-H ：以 M=1000K 取代 M=1024K 的进位方式；\n-T ：显示文件系统类型，连同该 partition 的 filesystem 名称 (例如 ext3) 也列出；\n-i ：不用硬盘容量，而以 inode 的数量来显示\n2）案例\n[root@bigdata111 ~]# df -h\nFilesystem Size Used Avail Use% Mounted on\n/dev/sda2 15G 3.5G 11G 26% /\ntmpfs 939M 224K 939M 1% /dev/shm\n/dev/sda1 190M 39M 142M 22% /boot\nmount/umount挂载/卸载 对于Linux用户来讲，不论有几个分区，分别分给哪一个目录使用，它总归就是一个根目录、一个独立且唯一的文件结构\nLinux中每个分区都是用来组成整个文件系统的一部分，她在用一种叫做“挂载”的处理方法，它整个文件系统中包含了一整套的文件和目录，并将一个分区和一个目录联系起来，要载入的那个分区将使它的存储空间在这个目录下获得。\n0**）挂载前准备（必须要有光盘或者已经连接镜像文件）**\n1**）挂载光盘语法：**\nmount [-t vfstype] [-o options] device dir\n（1）-t vfstype 指定文件系统的类型，通常不必指定。mount 会自动选择正确的类型。\n常用类型有：\n光盘或光盘镜像：iso9660\nDOS fat16文件系统：msdos\nWindows 9x fat32文件系统：vfat\nWindows NT ntfs文件系统：ntfs\nMount Windows文件网络共享：smbfs\nUNIX(LINUX) 文件网络共享：nfs\n（2）-o options 主要用来描述设备或档案的挂接方式。常用的参数有：\nloop：用来把一个文件当成硬盘分区挂接上系统\n　ro：采用只读方式挂接设备\n　rw：采用读写方式挂接设备\n　iocharset：指定访问文件系统所用字符集\n（3）device 要挂接(mount)的设备\n（4）dir设备在系统上的挂接点(mount point)\n2**）案例**\n（1）光盘镜像文件的挂载\n[root@bigdata111 ~]# mkdir /mnt/cdrom/ 建立挂载点\n[root@bigdata111 ~]# mount -t iso9660 /dev/cdrom /mnt/cdrom/ 设备/dev/cdrom挂载到 挂载点 ： /mnt/cdrom中\n[root@bigdata111 ~]# ll /mnt/cdrom/\n3**）卸载光盘语法：**\n[root@bigdata111 ~]# umount 设备文件名或挂载点\n4**）案例**\n[root@bigdata111 ~]# umount /mnt/cdrom\n5**）开机自动挂载语法：**\n[root@bigdata111 ~]# vi /etc/fstab\n添加红框中内容，保存退出。\n1.8 搜索查找类 1.2.2 grep 过滤查找及“|”管道符 0）管道符，“|”，表示将前一个命令的处理结果输出传递给后面的命令处理\n1）基本语法\ngrep+参数+查找内容+源文件\n参数：\n-c：只输出匹配行的计数。\n-I：不区分大小写(只适用于单字符)。\n-h：查询多文件时不显示文件名。\n-l：查询多文件时只输出包含匹配字符的文件名。\n-n：显示匹配行及行号。\n-s：不显示不存在或无匹配文本的错误信息。\n-v：显示不包含匹配文本的所有行。\n2）案例\n[root@bigdata111 opt]# ls | grep -n test\n4:test1\n5:test2\n1.2.3 which 文件搜索命令 1）基本语法：\n​ which 命令 （功能描述：搜索命令所在目录及别名信息）\n2）案例\n​ [root@bigdata111 opt]# which ls\n​ /bin/ls\n1.2.1 find 查找文件或者目录 1）基本语法：\n​ find [搜索范围] [匹配条件]\n2）案例\n（1）按文件名：根据名称查找/目录下的filename.txt文件。\n[root@bigdata111 ~]# find /opt/ -name *.txt\n（2）按拥有者：查找/opt目录下，用户名称为-user的文件\n[root@bigdata111 ~]# find /opt/ -user itstar\n​ （3）按文件大小：在/home目录下查找大于200m的文件（+n 大于 -n小于 n等于）\n[root@bigdata111 ~]find /home -size +204800\n1.9 进程线程类 进程是正在执行的一个程序或命令，每一个进程都是一个运行的实体，都有自己的地址空间，并占用一定的系统资源。\n1.9.1 ps查看系统中所有进程 1）基本语法：\n​ ps -aux （功能描述：查看系统中所有进程）\n2）功能说明\n​ USER：该进程是由哪个用户产生的\n​ PID：进程的ID号\n%CPU：该进程占用CPU资源的百分比，占用越高，进程越耗费资源；\n%MEM：该进程占用物理内存的百分比，占用越高，进程越耗费资源；\nVSZ：该进程占用虚拟内存的大小，单位KB；\nRSS：该进程占用实际物理内存的大小，单位KB；\nTTY：该进程是在哪个终端中运行的。其中tty1-tty7代表本地控制台终端，tty1-tty6是本地的字符界面终端，tty7是图形终端。pts/0-255代表虚拟终端。\nSTAT：进程状态。常见的状态有：R：运行、S：睡眠、T：停止状态、s：包含子进程、+：位于后台\nSTART：该进程的启动时间\nTIME：该进程占用CPU的运算时间，注意不是系统时间\nCOMMAND：产生此进程的命令名\n3）案例\n​ [root@bigdata111 datas]# ps -aux\n1.9.2 top查看系统健康状态 1）基本命令\n​ top [选项]\n​ （1）选项：\n​ -d 秒数：指定top命令每隔几秒更新。默认是3秒在top命令的交互模式当中可以执行的命令：\n-i：使top不显示任何闲置或者僵死进程。\n-p：通过指定监控进程ID来仅仅监控某个进程的状态。\n​ （2）操作选项：\nP： 以CPU使用率排序，默认就是此项\nM： 以内存的使用率排序\nN： 以PID排序\nq： 退出top\n​ （3）查询结果字段解释\n第一行信息为任务队列信息\n   内容 说明     12:26:46 系统当前时间   up 1 day, 13:32 系统的运行时间，本机已经运行1天 13小时32分钟   2 users 当前登录了两个用户   load average: 0.00, 0.00, 0.00 系统在之前1分钟，5分钟，15分钟的平均负载。一般认为小于1时，负载较小。如果大于1，系统已经超出负荷。    第二行为进程信息\n   Tasks: 95 total 系统中的进程总数     1 running 正在运行的进程数   94 sleeping 睡眠的进程   0 stopped 正在停止的进程   0 zombie 僵尸进程。如果不是0，需要手工检 查僵尸进程    第三行为CPU信息\n   Cpu(s): 0.1%us 用户模式占用的CPU百分比     0.1%sy 系统模式占用的CPU百分比   0.0%ni 改变过优先级的用户进程占用的CPU百分比   99.7%id 空闲CPU的CPU百分比   0.1%wa 等待输入/输出的进程的占用CPU百分比   0.0%hi 硬中断请求服务占用的CPU百分比   0.1%si 软中断请求服务占用的CPU百分比   0.0%st st（Steal time）虚拟时间百分比。就是当有虚拟机时，虚拟CPU等待实际CPU的时间百分比。    第四行为物理内存信息\n   Mem: 625344k total 物理内存的总量，单位KB     571504k used 已经使用的物理内存数量   53840k free 空闲的物理内存数量，我们使用的是虚拟机，总共只分配了628MB内存，所以只有53MB的空闲内存了   65800k buffers 作为缓冲的内存数量    第五行为交换分区（swap）信息\n   Swap: 524280k total 交换分区（虚拟内存）的总大小     0k used 已经使用的交互分区的大小   524280k free 空闲交换分区的大小   409280k cached 作为缓存的交互分区的大小    top命令第七行，各进程的监控：\n依次对应：\nPID — 进程id\nUSER — 进程所有者\nPR — 进程优先级\nNI — nice值。负值表示高优先级，正值表示低优先级\nVIRT — 进程使用的虚拟内存总量，单位kb。VIRT=SWAP+RES\nRES — 进程使用的、未被换出的物理内存大小，单位kb。RES=CODE+DATA\nSHR — 共享内存大小，单位kb\nS — 进程状态。D=不可中断的睡眠状态 R=运行 S=睡眠 T=跟踪/停止 Z=僵尸进程\n%CPU — 上次更新到现在的CPU时间占用百分比\n%MEM — 进程使用的物理内存百分比\nTIME+ — 进程使用的CPU时间总计，单位1/100秒\nCOMMAND — 进程名称（命令名/命令行）\n2）案例\n​ [root@bigdata111 itstar]# top -d 1\n[root@bigdata111 itstar]# top -i\n[root@bigdata111 itstar]# top -p 2575\n执行上述命令后，可以按P、M、N对查询出的进程结果进行排序。\n1.9.3 pstree查看进程树 1）基本语法：\n​ pstree [选项]\n​ 选项\n-p： 显示进程的PID\n-u： 显示进程的所属用户\n显示“-bash: pstree: command not found” 表示没有pstree的命令\nyum安装：\nyum -y install psmisc\n2）案例：\n​ [root@bigdata111 datas]# pstree -u\n[root@bigdata111 datas]# pstree -p\n1.9.4 kill终止进程 1）基本语法：\n​ kill -9 pid进程号\n​ 选项\n-9 表示强迫进程立即停止\n2）案例：\n​ 启动mysql程序\n​ 切换到root用户执行\n​ [root@bigdata111 桌面]# kill -9 5102\n1.9.5 netstat显示网络统计信息 1）基本语法：\n​ netstat -anp （功能描述：此命令用来显示整个系统目前的网络情况。例如目前的连接、数据包传递数据、或是路由表内容）\n​ 选项：\n​ -an 按一定顺序排列输出\n​ -p 表示显示哪个进程在调用\n​ -nltp 查看tcp协议进程端口号\n2）案例\n查看端口50070的使用情况\n[root@bigdata111 hadoop-2.1.2]# netstat -anp | grep 50070\ntcp 0 0 0.0.0.0:50070 0.0.0.0:* LISTEN 6816/java\n​ 端口号 进程号\n1.9.6 前后台进程切换 1）基本语法：\nfg %1 （功能描述：把后台进程转换成前台进程）\nctrl+z bg %1 （功能描述：把前台进程发到后台\n1.9.7 防火墙 查看：systemctl status firewalld.service\n关闭：systemctl stop firewalld.service\n禁止启动：systemctl disable firewalld.service\n1.10 压缩和解压类 1.10.1 gzip/gunzip压缩 1）基本语法：\ngzip+文件 （功能描述：压缩文件，只能将文件压缩为*.gz文件）\ngunzip+文件.gz （功能描述：解压缩文件命令）\n2）特点：\n（1）只能压缩文件不能压缩目录\n（2）不保留原来的文件\n3）案例\n（1）gzip压缩\n[root@bigdata111 opt]# ls\ntest.java\n[root@bigdata111 opt]# gzip test.java\n[root@bigdata111 opt]# ls\ntest.java.gz\n（2）gunzip解压缩文件\n[root@bigdata111 opt]# gunzip test.java.gz\n[root@bigdata111 opt]# ls\ntest.java\n1.10.2 zip/unzip压缩 1）基本语法：\nzip + 参数 + XXX.zip + 将要压缩的内容 （功能描述：压缩文件和目录的命令，window/linux通用且可以压缩目录且保留源文件）\n参数：\n-r 压缩目录\n（1）压缩 1.txt 和2.txt，压缩后的名称为mypackage.zip [root@bigdata111 opt]# zip test.zip test1.java test.java adding: test1.java (stored 0%) adding: test.java (stored 0%) [root@bigdata111 opt]# ls test1.java test.java test.zip （2）解压 mypackage.zip [root@bigdata111 opt]# unzip test.zip Archive: test.zip extracting: test1.java extracting: test.java ​ [root@bigdata111 opt]# ls test1.java test.java test.zip  1.10.3 tar打包 1）基本语法：\ntar + 参数 + XXX.tar.gz + 将要打包进去的内容 （功能描述：打包目录，压缩后的文件格式.tar.gz）\n参数：\n-c 产生.tar打包文件\n-v 显示详细信息\n-f 指定压缩后的文件名\n-z 打包同时压缩\n-x 解包.tar文件\n（1）压缩：tar -zcvf XXX.tar.gz n1.txt n2.txt ​ 压缩多个文件 [root@bigdata111 opt]# tar -zcvf test.tar.gz test1.java test.java test1.java test.java [root@bigdata111 opt]# ls test1.java test.java test.tar.gz 压缩目录 [root@bigdata111 opt]# tar -zcvf test.java.tar.gz test1 test1/ test1/hello test1/test1.java test1/test/ test1/test/test.java [root@hadoop106 opt]# ls test1 test.java.tar.gz （2）解压：tar -zxvf XXX.tar.gz ​ 解压到当前目录 [root@bigdata111 opt]# tar -zxvf test.tar.gz 解压到/opt目录 [root@bigdata111 opt]# tar -zxvf test.tar.gz -C /opt  1.11 后台服务管理类 1.11.1 service后台服务管理 1）service network status 查看指定服务的状态\n2）service network stop 停止指定服务\n3）service network start 启动指定服务\n4）service network restart 重启指定服务\n5）service \u0026ndash;status-all 查看系统中所有的后台服务\n1.11.2 chkconfig设置后台服务的自启配置 1）chkconfig 查看所有服务器自启配置\n2）chkconfig iptables off 关掉指定服务的自动启动\n3）chkconfig iptables on 开启指定服务的自动启动\n1.12 crond系统定时任务 1.12.1 crond服务管理 [root@bigdata111 ~]# service crond restart （重新启动服务）\n1.12.2 crontab定时任务设置 1）基本语法\ncrontab [选项]\n选项：\n-e： 编辑crontab定时任务\n-l： 查询crontab任务\n-r： 删除当前用户所有的crontab任务\n2）参数说明\n​ [root@bigdata111 ~]# crontab -e\n（1）进入crontab编辑界面。会打开vim编辑你的工作。\n* * * * * 执行的任务\n   项目 含义 范围     第一个“*” 一小时当中的第几分钟 0-59   第二个“*” 一天当中的第几小时 0-23   第三个“*” 一个月当中的第几天 1-31   第四个“*” 一年当中的第几月 1-12   第五个“*” 一周当中的星期几 0-7（0和7都代表星期日）    （2）特殊符号\n   特殊符号 含义     * 代表任何时间。比如第一个“*”就代表一小时中每分钟都执行一次的意思。   ， 代表不连续的时间。比如“0 8,12,16 * * * 命令”，就代表在每天的8点0分，12点0分，16点0分都执行一次命令   - 代表连续的时间范围。比如“0 5 * * 1-6命令”，代表在周一到周六的凌晨5点0分执行命令   */n 代表每隔多久执行一次。比如“*/10 * * * * 命令”，代表每隔10分钟就执行一遍命令    （3）特定时间执行命令\n   时间 含义     45 22 * * * 命令 在22点45分执行命令   0 17 * * 1 命令 每周1 的17点0分执行命令   0 5 1,15 * * 命令 每月1号和15号的凌晨5点0分执行命令   40 4 * * 1-5 命令 每周一到周五的凌晨4点40分执行命令   */10 4 * * * 命令 每天的凌晨4点，每隔10分钟执行一次命令   0 0 1,15 * 1 命令 每月1号和15号，每周1的0点0分都会执行命令。注意：星期几和几号最好不要同时出现，因为他们定义的都是天。非常容易让管理员混乱。    3）案例：\n*/5 * * * * /bin/echo ”11” \u0026raquo; /tmp/test\n*/1 * * * * /bin/echo ”11” \u0026raquo; /opt/TZ/ITSTAR\n二 RPM 2.1 概述 RPM（RedHat Package Manager），Rethat软件包管理工具，类似windows里面的setup.exe\n是Linux这系列操作系统里面的打包安装工具，它虽然是RedHat的标志，但理念是通用的。\nRPM包的名称格式\nApache-1.3.23-11.i386.rpm\n- “apache” 软件名称\n- “1.3.23-11”软件的版本号，主版本和此版本\n- “i386”是软件所运行的硬件平台\n- “rpm”文件扩展名，代表RPM包\n2.2 常用命令 2.2.1 查询（rpm -qa） 1）基本语法：\nrpm -qa （功能描述：查询所安装的所有rpm软件包）\n过滤\nrpm -qa | grep rpm软件包\n2）案例\n[root@bigdata111 Packages]# rpm -qa |grep firefox\nfirefox-45.0.1-1.el6.centos.x86_64\n2.2.2 卸载（rpm -e） 1）基本语法：\n（1）rpm -e RPM软件包\n或者（2） rpm -e \u0026ndash;nodeps 软件包\n\u0026ndash;nodeps 如果该RPM包的安装依赖其它包，即使其它包没装，也强迫安装。\n2）案例\n[root@bigdata111 Packages]# rpm -e firefox\n2.2.3 安装（rpm -ivh） 1）基本语法：\n​ rpm -ivh RPM包全名\n​ -i=install，安装\n​ -v=verbose，显示详细信息\n​ -h=hash，进度条\n​ \u0026ndash;nodeps，不检测依赖进度\n [root@bigdata111 Packages]# pwd /media/CentOS_6.8_Final/Packages [root@bigdata111 Packages]# rpm -ivh firefox-45.0.1-1.el6.centos.x86_64.rpm warning: firefox-45.0.1-1.el6.centos.x86_64.rpm: Header V3 RSA/SHA1 Signature, key ID c105b9de: NOKEY Preparing... ########################################### [100%] 1:firefox ########################################### [100%]  搜索  find[匹配范围][条件] 搜索文件 grep 管道符  进程  ps -aux 查看系统中的进程 top查看系统健康状态 kill 进程 kill -9 直接杀死进程  Linux定时任务Crontab   crontab -e 编辑定时任务\n  crontab -l 查询定时任务\n  crontable -r 删除定时任务\n  常用命令   ctrl+c 停止进程\n  ctel+l 清屏\n  tab 自动补全\n  创建硬链接 ln 文件名 链接名\n  rmdir 名字 删除空目录\n  cd - 返回上一次所在的用户\n  cd -P 跳转到实际路径\n  mv 目标名 文件名（路径） 改名，移动 若同目录下有该文件名，则改名，不同目录下，移动\n  cat -A 显示一些空白字符\n  cat -T tab键变为^I\n  cat -E 每行后加$\n  tac 倒着查看（其他属性和cat相同）\n  less + 要查看的文件 /向下查询 ？想上搜寻 q离开 上下箭头翻页\n  head -n m 文件名 显示前 m 行\n  tail -n m 文件名 显示后m行\n   重定向 例如： ls \u0026gt; test 将ls查看到的文件写入test （多次使用会覆盖）\n   ls \u0026raquo; test 追加（多次使用不会覆盖）\n  echo 文本 \u0026raquo; test 追加至test\n  echo $PATH 环境变量符号，查看环境变量\n  ","id":14,"section":"posts","summary":"linux文件权限管理 共10位 r 读 w 写 x 执行 第1位为：d目录 -文件 l软连接代码 第2-5位为：文件所属者权限 第5-8位为：所属组 第8-10位为","tags":["Linux"],"title":"Linux命令","uri":"https://brightsails.github.io/2020/02/linux%E5%91%BD%E4%BB%A4/","year":"2020"},{"content":"配置linux ip linux下查看ip指令：\nip addr  配置ip：\n1.进入 cd etc/sysconfig/network-scripts\n2.打开文件 vi ifcfg-ens33\n3.编辑文件 4.重新启动 service network restart  vmware及本地配置 ","id":15,"section":"posts","summary":"配置linux ip linux下查看ip指令： ip addr 配置ip： 1.进入 cd etc/sysconfig/network-scripts 2.打开文件 vi ifcfg-ens33 3.编辑文件 4.重新启动 service network restart vmware及本地配置","tags":["环境搭建","Linux"],"title":"linux网络配置","uri":"https://brightsails.github.io/2020/01/linux%E7%BD%91%E7%BB%9C%E9%85%8D%E7%BD%AE/","year":"2020"},{"content":"本地环境操作： 1.windows下载hugo环境 https://gohugo.io/getting-started/installing/  2.下载git https://www.git-scm.com/download/  3.下载node https://nodejs.org/zh-cn/download/  4.安装完成后，初始化博客 hugo new site myblog  5.新建文章 hugo new post/blog.md  6.选定主题 https://themes.gohugo.io/ 将选择的主题克隆至本地themes中，打开其中的exampleSite文件，将配置文件复制到hugo主目录  7.本地运行博客 hugo server -t pure  8.生成public文件 hugo --theme=pure --baseUrl=\u0026quot;https://brightsails.github.io\u0026quot;  9.进入文件，并且对git进行初始化，上传至github仓库 cd public/ //git初始化 git init //将文件增加至git git add . //本次提交的描述 git commit -m \u0026quot;第er次commit\u0026quot; //git用户设置 git config --global user.name\u0026quot;brightsails\u0026quot; //git用户设置 git config --global user.email\u0026quot;**********\u0026quot; //将github仓库与本地文件关联 git remote add origin https://github.com/BrightSails/brightsails.github.io.git //上传 git push -u origin master  10.更新GitHub仓库 主页面下： hugo public页面下： git add -A git commit -m \u0026quot;描述\u0026quot; git push -u origin master  11.若出现错误 To github.com:peTzxz/Property-management-system ! [rejected] master -\u0026gt; master (fetch first) error: failed to push some refs to 'git@github.com:peTzxz/Property-management-system' hint: Updates were rejected because the remote contains work that you do hint: not have locally. This is usually caused by another repository pushing hint: to the same ref. You may want to first integrate the remote changes hint: (e.g., 'git pull ...') before pushing again. hint: See the 'Note about fast-forwards' in 'git push --help' for details.  出现这个问题是因为github中的README.md文件不在本地代码目录中，可以通过如下命令进行代码合并，解决方法：\ngit pull --rebase origin master git push origin master  GitHub操作： 1.新建博客存储仓库 仓库有名字为github名字且全为小写+github.io\n2.新建图床仓库 仓库有名字随便写\n3.配置图床  下载picgo软件  https://picgo.github.io/PicGo-Doc/zh/guide/   创建token  将显示出来的token保存（只出现一次）\n 打开picgo，填入相关信息  4.图片的链接   打开GitHub建立的图床仓库，复制地址，将其中的blob改为raw，例如：\nhttps://github.com/BrightSails/pic/raw/master/hugoblog_configpic.png    博客的优化 1.pure\\layouts\\partials\\footer.html中修改：\n2.pure\\layouts\\partials\\_widgets\\board.html中修改：\n\u0026amp;\u0026amp;\n3.pure\\i18n\\zh.yaml中修改：\n","id":16,"section":"posts","summary":"\u003ch2 id=\"本地环境操作\"\u003e本地环境操作：\u003c/h2\u003e\n\u003ch4 id=\"1windows下载hugo环境\"\u003e1.windows下载hugo环境\u003c/h4\u003e\n\u003cpre\u003e\u003ccode\u003ehttps://gohugo.io/getting-started/installing/\n\u003c/code\u003e\u003c/pre\u003e","tags":["环境搭建"],"title":"hugo博客的搭建","uri":"https://brightsails.github.io/2020/01/hugo%E5%8D%9A%E5%AE%A2%E7%9A%84%E6%90%AD%E5%BB%BA/","year":"2020"},{"content":"正则表达式 [abc]：a、b或c（简单类） [\u0026ldquo;^\u0026quot;abc]：任何字符，除了a、b或c（否定） [a-zA-Z]：a到z或A到Z，两头的字母包括在内（范围） [0-9]：0到9的字符都包括\n[a-d[m-p]]：a到d或m到p：[a-dm-p]（并集）\n[a-z\u0026amp;\u0026amp;[def]]：d、e或f（交集 [a-z\u0026amp;\u0026amp;[”^“bc]]：a到z，除了b和c：[ad-z]（减去） [a-z8\u0026amp;^m-p]]：a到z，而非m到p：[a-1q-z ] 减去\n\\d 数字：[0-9], \\是转义字符，若表示\\d,则需要写为\\\\d\n\\w 单词字符： [a-zA-z_0-9]\n\\D非数字\n\\W非单词字符\n\\s：空白字符：[ \\t\\n\\x0B\\f\\r]\n.：点代表单个字符\nX？ X，一次或一次也没有 X* X，零次或多次 X+ X，一次或多次 x{n} X，怡好n次 X{n,} X，至少n次 X{n,m} X，至少n次，但是不超过m次\n[abc]{5,15} 5-15位都是abc\n","id":17,"section":"posts","summary":"正则表达式 [abc]：a、b或c（简单类） [\u0026ldquo;^\u0026quot;abc]：任何字符，除了a、b或c（否定） [a-zA-Z]：a到z或A到Z，两头的字母包括在内（范围） [0-9]：0到9的字符都包括\n[a-d[m-p]]：a到d或m到p：[a-dm-p]（并集）\n[a-z\u0026amp;\u0026amp;[def]]：d、e或f（交集 [a-z\u0026amp;\u0026amp;[”^“bc]]：a到z，除了b和c：[ad-z]（减去） [a-z8\u0026amp;^m-p]]：a到z，而非m到p：[a-1q-z ] 减去\n\\d 数字：[0-9], \\是转义字符，若表示\\d,则需要写为\\\\d\n\\w 单词字符： [a-zA-z_0-9]\n\\D非数字\n\\W非单词字符\n\\s：空白字符：[ \\t\\n\\x0B\\f\\r]\n.：点代表单个字符\nX？ X，一次或一次也没有 X* X，零次或多次 X+ X，一次或多次 x{n} X，怡好n次 X{n,} X，至少n次 X{n,m} X，至少n次，但是不超过m次\n[abc]{5,15} 5-15位都是abc","tags":null,"title":"","uri":"https://brightsails.github.io/1/01/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/","year":"0001"}],"tags":[{"title":"GDB","uri":"https://brightsails.github.io/tags/gdb/"},{"title":"hadoop","uri":"https://brightsails.github.io/tags/hadoop/"},{"title":"hdfs","uri":"https://brightsails.github.io/tags/hdfs/"},{"title":"Java基础","uri":"https://brightsails.github.io/tags/java%E5%9F%BA%E7%A1%80/"},{"title":"Linux","uri":"https://brightsails.github.io/tags/linux/"},{"title":"makefile","uri":"https://brightsails.github.io/tags/makefile/"},{"title":"环境搭建","uri":"https://brightsails.github.io/tags/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"}]}